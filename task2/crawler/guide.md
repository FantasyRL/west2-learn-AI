# çˆ¬è™«é¡¹ç›®å¼€å‘æŒ‡å—

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäº Python çš„é€šç”¨çˆ¬è™«å¹³å°ï¼Œæ”¯æŒå¤šç§çˆ¬è™«ä»»åŠ¡çš„ç»Ÿä¸€ç®¡ç†å’Œè°ƒåº¦ã€‚é‡‡ç”¨ **DDD (é¢†åŸŸé©±åŠ¨è®¾è®¡) + Clean Architecture** æ¶æ„ï¼Œé€šè¿‡ **Protocol Buffers** å®šä¹‰ API æ¥å£ï¼Œæ•°æ®æŒä¹…åŒ–åˆ° PostgreSQL æ•°æ®åº“ã€‚

### æ ¸å¿ƒç‰¹æ€§

- **é¢†åŸŸé©±åŠ¨è®¾è®¡ (DDD)**ï¼šæ¸…æ™°çš„ä¸šåŠ¡é¢†åŸŸåˆ’åˆ†å’Œåˆ†å±‚æ¶æ„
- **å¯å¤ç”¨çš„çˆ¬è™«å¼•æ“**ï¼šæŠ½è±¡å‡ºé€šç”¨çš„çˆ¬è™«ç»„ä»¶ï¼Œæ”¯æŒå¿«é€Ÿæ‰©å±•æ–°çš„çˆ¬è™«ä»»åŠ¡
- **IDL é©±åŠ¨å¼€å‘**ï¼šé€šè¿‡ Protocol Buffers å®šä¹‰æ¥å£è§„èŒƒï¼Œè‡ªåŠ¨ç”Ÿæˆ API ä»£ç 
- **æ•°æ®æŒä¹…åŒ–**ï¼šä½¿ç”¨ PostgreSQL + SQLAlchemy ORM
- **ä»£ç ç”Ÿæˆå·¥å…·**ï¼šç±»ä¼¼ GORM-Gen çš„æ¨¡å‹ç”Ÿæˆå™¨
- **çµæ´»é…ç½®**ï¼šæ”¯æŒå¤šç§çˆ¬è™«ç­–ç•¥ï¼ˆHTTPã€Seleniumã€API è°ƒç”¨ï¼‰

## æŠ€æœ¯é€‰å‹

### 1. æ¶æ„æ¨¡å¼
**DDD (é¢†åŸŸé©±åŠ¨è®¾è®¡) + Clean Architecture**

**æ¶æ„ç†ç”±**ï¼š
- æ¸…æ™°çš„åˆ†å±‚ç»“æ„ï¼ŒèŒè´£åˆ†æ˜
- ä¸šåŠ¡é€»è¾‘ä¸æŠ€æœ¯å®ç°è§£è€¦
- æ˜“äºæµ‹è¯•å’Œç»´æŠ¤
- æ”¯æŒä¸šåŠ¡å¿«é€Ÿè¿­ä»£

### 2. IDL å®šä¹‰
**Protocol Buffers (protobuf)**

**é€‰å‹ç†ç”±**ï¼š
- å¼ºç±»å‹çº¦æŸï¼Œæ¥å£å®šä¹‰æ¸…æ™°
- è·¨è¯­è¨€æ”¯æŒ
- è‡ªåŠ¨ç”Ÿæˆä»£ç 
- ä¾¿äºå‰åç«¯åä½œ

### 3. ORM æ¡†æ¶
**SQLAlchemy 2.0**

**é€‰å‹ç†ç”±**ï¼š
- Python ç”Ÿæ€æœ€æˆç†Ÿçš„ ORM
- æ”¯æŒå¼‚æ­¥æ“ä½œï¼ˆasyncioï¼‰
- ç±»å‹æç¤ºæ”¯æŒ

**è‡ªç ”ä»£ç ç”Ÿæˆå·¥å…·**ï¼š`pkg/sql-gen` (ç±»ä¼¼ GORM-Gen)
- ä»æ•°æ®åº“è¡¨ç»“æ„è‡ªåŠ¨ç”Ÿæˆ SQLAlchemy æ¨¡å‹
- é…ç½®é©±åŠ¨ï¼Œä¸€é”®ç”Ÿæˆ
- æ™ºèƒ½ç±»å‹æ˜ å°„

### 4. çˆ¬è™«åº“
- **httpx**: å¼‚æ­¥ HTTP å®¢æˆ·ç«¯ï¼ˆæ¨èï¼‰
- **lxml**: XPath è§£æ
- **selenium**: æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼ˆçŸ¥ä¹ç­‰åçˆ¬åœºæ™¯ï¼‰

### 5. Web æ¡†æ¶
**FastAPI** (å¯é€‰ï¼Œç”¨äºæä¾› HTTP API)

**é€‰å‹ç†ç”±**ï¼š
- åŸç”Ÿæ”¯æŒ async/await
- è‡ªåŠ¨ç”Ÿæˆ OpenAPI æ–‡æ¡£
- ä¸ Pydantic æ·±åº¦é›†æˆ

## é¡¹ç›®ç»“æ„ (DDD + Clean Architecture)

```
crawler/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ guide.md                    # æœ¬æ–‡æ¡£
â”œâ”€â”€ requirements.txt            # ä¾èµ–åŒ…
â”œâ”€â”€ Makefile                    # æ„å»ºè„šæœ¬
â”‚
â”œâ”€â”€ config/                     # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.yaml            # ç»Ÿä¸€é…ç½®ï¼ˆæ•°æ®åº“ã€AIç­‰ï¼‰
â”‚
â”œâ”€â”€ idl/                        # IDL æ¥å£å®šä¹‰
â”‚   â””â”€â”€ crawler.proto          # Protocol Buffers å®šä¹‰
â”‚
â”œâ”€â”€ api/                        # API å±‚ï¼ˆä» IDL ç”Ÿæˆï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ generated/             # è‡ªåŠ¨ç”Ÿæˆçš„ API ä»£ç 
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ crawler_pb2.py     # Protobuf æ¶ˆæ¯
â”‚       â””â”€â”€ crawler_pb2_grpc.py # gRPC æœåŠ¡
â”‚
â”œâ”€â”€ internal/                   # å†…éƒ¨ä¸šåŠ¡é€»è¾‘ï¼ˆæ ¸å¿ƒå±‚ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ application/           # åº”ç”¨æœåŠ¡å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ fzu_service.py     # ç¦å¤§é€šçŸ¥åº”ç”¨æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ zhihu_service.py   # çŸ¥ä¹è¯é¢˜åº”ç”¨æœåŠ¡
â”‚   â”‚   â””â”€â”€ ospp_service.py    # å¼€æºä¹‹å¤åº”ç”¨æœåŠ¡
â”‚   â”‚
â”‚   â”œâ”€â”€ domain/                # é¢†åŸŸå±‚ï¼ˆæ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ entity/            # é¢†åŸŸå®ä½“
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ fzu_notice.py  # ç¦å¤§é€šçŸ¥å®ä½“
â”‚   â”‚   â”‚   â”œâ”€â”€ zhihu_topic.py # çŸ¥ä¹è¯é¢˜å®ä½“
â”‚   â”‚   â”‚   â””â”€â”€ ospp_project.py # å¼€æºä¹‹å¤å®ä½“
â”‚   â”‚   â”œâ”€â”€ repository/        # ä»“å‚¨æ¥å£ï¼ˆæŠ½è±¡ï¼‰
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ fzu_repository.py
â”‚   â”‚   â”‚   â”œâ”€â”€ zhihu_repository.py
â”‚   â”‚   â”‚   â””â”€â”€ ospp_repository.py
â”‚   â”‚   â””â”€â”€ service/           # é¢†åŸŸæœåŠ¡
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ crawler_service.py
â”‚   â”‚
â”‚   â””â”€â”€ infra/                 # åŸºç¡€è®¾æ–½å±‚
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ persistence/       # æŒä¹…åŒ–å®ç°
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ database.py    # æ•°æ®åº“è¿æ¥
â”‚       â”‚   â”œâ”€â”€ fzu_repo_impl.py
â”‚       â”‚   â”œâ”€â”€ zhihu_repo_impl.py
â”‚       â”‚   â””â”€â”€ ospp_repo_impl.py
â”‚       â””â”€â”€ external/          # å¤–éƒ¨æœåŠ¡
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ crawler_client.py
â”‚
â”œâ”€â”€ models/                     # æ•°æ®åº“æ¨¡å‹ï¼ˆä» sql-gen ç”Ÿæˆï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                # åŸºç¡€æ¨¡å‹
â”‚   â”œâ”€â”€ users.py               # ç¤ºä¾‹æ¨¡å‹
â”‚   â””â”€â”€ ...                    # å…¶ä»–ç”Ÿæˆçš„æ¨¡å‹
â”‚
â”œâ”€â”€ pkg/                        # å¯å¤ç”¨åŒ…
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ crawler/               # çˆ¬è™«å¼•æ“åŒ…
â”‚   â”‚   â”œâ”€â”€ __init__.py
## æ¶æ„åˆ†å±‚è¯´æ˜

### 1. API å±‚ (`api/`)
- ä» IDL (Protocol Buffers) è‡ªåŠ¨ç”Ÿæˆ
- å®šä¹‰äº†æ‰€æœ‰çš„æ¥å£å¥‘çº¦
- åŒ…å«è¯·æ±‚/å“åº”æ¶ˆæ¯å®šä¹‰

### 2. åº”ç”¨æœåŠ¡å±‚ (`internal/application/`)
- ç¼–æ’ä¸šåŠ¡æµç¨‹
- è°ƒç”¨é¢†åŸŸæœåŠ¡å’Œä»“å‚¨
- å¤„ç†äº‹åŠ¡è¾¹ç•Œ
- æ•°æ®è½¬æ¢ï¼ˆDTO â†” Entityï¼‰

### 3. é¢†åŸŸå±‚ (`internal/domain/`)
- **Entity**: é¢†åŸŸå®ä½“ï¼ŒåŒ…å«ä¸šåŠ¡é€»è¾‘
- **Repository**: ä»“å‚¨æ¥å£ï¼ˆæŠ½è±¡ï¼‰ï¼Œå®šä¹‰æ•°æ®è®¿é—®å¥‘çº¦
- **Service**: é¢†åŸŸæœåŠ¡ï¼Œå¤„ç†è·¨å®ä½“çš„ä¸šåŠ¡é€»è¾‘

### 4. åŸºç¡€è®¾æ–½å±‚ (`internal/infra/`)
- **Persistence**: ä»“å‚¨æ¥å£çš„å…·ä½“å®ç°
- **External**: å¤–éƒ¨æœåŠ¡è°ƒç”¨ï¼ˆå¦‚çˆ¬è™«å®¢æˆ·ç«¯ï¼‰
- æ•°æ®åº“è¿æ¥ã€é…ç½®åŠ è½½ç­‰

### 5. æ¨¡å‹å±‚ (`models/`)
- æ•°æ®åº“æ¨¡å‹ï¼ˆSQLAlchemyï¼‰
- é€šè¿‡ `pkg/sql-gen` å·¥å…·è‡ªåŠ¨ç”Ÿæˆ
- ä¸é¢†åŸŸå®ä½“åˆ†ç¦»ï¼Œéµå¾ªå…³æ³¨ç‚¹åˆ†ç¦»åŸåˆ™

### 6. åŒ…å±‚ (`pkg/`)
- å¯å¤ç”¨çš„ç‹¬ç«‹åŒ…
- çˆ¬è™«å¼•æ“ï¼ˆ`crawler`ï¼‰
- æ¨¡å‹ç”Ÿæˆå·¥å…·ï¼ˆ`sql-gen`ï¼‰

## æ ¸å¿ƒè®¾è®¡

### 1. çˆ¬è™«åŸºç±»è®¾è®¡ (pkg/crawler/base_crawler.py)
â”‚   â””â”€â”€ sql-gen/               # æ¨¡å‹ç”Ÿæˆå·¥å…·ï¼ˆç±»ä¼¼ GORM-Genï¼‰
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py            # ç”Ÿæˆå™¨å…¥å£
â”‚       â”œâ”€â”€ config.yaml        # ç”Ÿæˆå™¨é…ç½®
â”‚       â”œâ”€â”€ config_loader.py   # é…ç½®åŠ è½½
â”‚       â”œâ”€â”€ db_inspector.py    # æ•°æ®åº“æ£€æŸ¥
â”‚       â”œâ”€â”€ model_generator.py # æ¨¡å‹ç”Ÿæˆå™¨
â”‚       â”œâ”€â”€ requirements.txt   # ç”Ÿæˆå™¨ä¾èµ–
â”‚       â”œâ”€â”€ README.md          # ä½¿ç”¨æ–‡æ¡£
â”‚       â””â”€â”€ generated_models/  # ç”Ÿæˆçš„æ¨¡å‹è¾“å‡ºç›®å½•
â”‚
â”œâ”€â”€ tests/                      # æµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unit/                  # å•å…ƒæµ‹è¯•
â”‚   â””â”€â”€ integration/           # é›†æˆæµ‹è¯•
â”‚
â””â”€â”€ scripts/                    # è„šæœ¬
    â”œâ”€â”€ generate_proto.sh      # ç”Ÿæˆ Protobuf ä»£ç 
    â””â”€â”€ run_crawler.py         # è¿è¡Œçˆ¬è™«è„šæœ¬
```

## æ ¸å¿ƒè®¾è®¡

### 1. çˆ¬è™«åŸºç±»è®¾è®¡ (core/base_crawler.py)

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any
import logging

class BaseCrawler(ABC):
    """çˆ¬è™«æŠ½è±¡åŸºç±»"""
    
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
    
    @abstractmethod
    async def fetch(self, **kwargs) -> List[Dict[str, Any]]:
        """
        è·å–åŸå§‹æ•°æ®
        è¿”å›: åŸå§‹æ•°æ®åˆ—è¡¨
        """
        pass
    
    @abstractmethod
    async def parse(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æå•æ¡æ•°æ®
        è¿”å›: è§£æåçš„ç»“æ„åŒ–æ•°æ®
        """
        pass
    
    async def run(self, **kwargs) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå®Œæ•´çš„çˆ¬è™«æµç¨‹
        è¿”å›: è§£æåçš„æ•°æ®åˆ—è¡¨
        """
        try:
            # 1. è·å–æ•°æ®
            raw_data_list = await self.fetch(**kwargs)
            self.logger.info(f"Fetched {len(raw_data_list)} items")
            
            # 2. è§£ææ•°æ®
            parsed_data_list = []
            for raw_data in raw_data_list:
                try:
                    parsed = await self.parse(raw_data)
                    parsed_data_list.append(parsed)
                except Exception as e:
                    self.logger.error(f"Parse error: {e}")
                    continue
            
            return parsed_data_list
            
        except Exception as e:
            self.logger.error(f"Crawler run error: {e}")
            raise
```

**æ³¨æ„**: 
- åŸºç±»ä¸è´Ÿè´£æ•°æ®æŒä¹…åŒ–ï¼ˆéµå¾ªå•ä¸€èŒè´£ï¼‰
- æŒä¹…åŒ–ç”±ä»“å‚¨å±‚ï¼ˆRepositoryï¼‰è´Ÿè´£
- çˆ¬è™«åªè´Ÿè´£æ•°æ®çš„è·å–å’Œè§£æ

### 2. HTTP çˆ¬è™«åŸºç±» (pkg/crawler/http_crawler.py)

```python
from typing import Optional
import httpx
from core.base_crawler import BaseCrawler

class HttpCrawler(BaseCrawler):
    """HTTP çˆ¬è™«åŸºç±»"""
    
    def __init__(self, db_session: AsyncSession, 
                 headers: Optional[Dict[str, str]] = None,
                 timeout: int = 30):
        super().__init__(db_session)
        self.headers = headers or self._default_headers()
        self.timeout = timeout
        self.client = None
    
    def _default_headers(self) -> Dict[str, str]:
        return {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"
        }
    
    async def __aenter__(self):
        self.client = httpx.AsyncClient(
            headers=self.headers,
            timeout=self.timeout,
            follow_redirects=True
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.client:
            await self.client.aclose()
    
    async def get(self, url: str, **kwargs) -> httpx.Response:
        """å‘é€ GET è¯·æ±‚"""
        if not self.client:
            raise RuntimeError("Client not initialized. Use 'async with'")
        return await self.client.get(url, **kwargs)
    
    async def post(self, url: str, **kwargs) -> httpx.Response:
        """å‘é€ POST è¯·æ±‚"""
        if not self.client:
            raise RuntimeError("Client not initialized. Use 'async with'")
        return await self.client.post(url, **kwargs)
```

### 3. Selenium çˆ¬è™«åŸºç±» (core/selenium_crawler.py)

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from core.base_crawler import BaseCrawler

class SeleniumCrawler(BaseCrawler):
    """Selenium çˆ¬è™«åŸºç±»"""
    
    def __init__(self, db_session: AsyncSession, headless: bool = True):
        super().__init__(db_session)
        self.headless = headless
        self.driver = None
    
    def _init_driver(self):
        """åˆå§‹åŒ– Chrome é©±åŠ¨"""
        options = Options()
        if self.headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
    
    def __enter__(self):
        self._init_driver()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.driver:
            self.driver.quit()
    
    def get_page(self, url: str):
        """è®¿é—®é¡µé¢"""
        if not self.driver:
            raise RuntimeError("Driver not initialized. Use 'with'")
        self.driver.get(url)
```

### 4. æ•°æ®è§£æå™¨ (core/parser.py)

```python
from lxml import etree
from typing import List, Union

class Parser:
    """æ•°æ®è§£æå·¥å…·ç±»"""
    
    @staticmethod
    def xpath(html: str, xpath_expr: str) -> List[str]:
        """XPath è§£æ"""
        tree = etree.HTML(html)
        return tree.xpath(xpath_expr)
    
    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ (å»é™¤ç©ºç™½ã€æ¢è¡Œç­‰)"""
        return ' '.join(text.split()).strip()
    
    @staticmethod
    def extract_links(html: str, base_url: str = "") -> List[str]:
        """æå–æ‰€æœ‰é“¾æ¥"""
        tree = etree.HTML(html)
        links = tree.xpath('//a/@href')
        if base_url:
            from urllib.parse import urljoin
            links = [urljoin(base_url, link) for link in links]
        return links
```

## ä» IDL ç”Ÿæˆ API ä»£ç 

### 1. Protocol Buffers æ¥å£å®šä¹‰

æ–‡ä»¶ä½ç½®: `idl/crawler.proto`

### 2. ç”Ÿæˆ Python ä»£ç 

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ protoc å‘½ä»¤

```bash
# å®‰è£… grpcio-tools
pip install grpcio-tools

# ç”Ÿæˆ Python ä»£ç 
python -m grpc_tools.protoc \
    -I./idl \
    --python_out=./api/generated \
    --grpc_python_out=./api/generated \
    --pyi_out=./api/generated \
    ./idl/crawler.proto
```

#### æ–¹æ³•äºŒï¼šä½¿ç”¨ Makefileï¼ˆæ¨èï¼‰

åœ¨ `Makefile` ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š

```makefile
# ç”Ÿæˆ API ä»£ç 
.PHONY: proto
proto:
	python -m grpc_tools.protoc \
		-I./idl \
		--python_out=./api/generated \
		--grpc_python_out=./api/generated \
		--pyi_out=./api/generated \
		./idl/crawler.proto
	@echo "âœ“ Protobuf ä»£ç ç”Ÿæˆå®Œæˆ"

# ç”Ÿæˆæ•°æ®åº“æ¨¡å‹
.PHONY: model
model:
	python pkg/sql-gen/main.py --config $(GEN_CONFIG_PATH)

# åŒæ—¶ç”Ÿæˆ API å’Œæ¨¡å‹
.PHONY: gen
gen: proto model
	@echo "âœ“ æ‰€æœ‰ä»£ç ç”Ÿæˆå®Œæˆ"
```

è¿è¡Œï¼š
```bash
make proto    # ç”Ÿæˆ API ä»£ç 
make model    # ç”Ÿæˆæ•°æ®åº“æ¨¡å‹
make gen      # ç”Ÿæˆæ‰€æœ‰ä»£ç 
```

#### æ–¹æ³•ä¸‰ï¼šä½¿ç”¨è„šæœ¬

åˆ›å»º `scripts/generate_proto.sh`:

```bash
#!/bin/bash

echo "ğŸš€ å¼€å§‹ç”Ÿæˆ Protobuf ä»£ç ..."

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p api/generated

# ç”Ÿæˆä»£ç 
python -m grpc_tools.protoc \
    -I./idl \
    --python_out=./api/generated \
    --grpc_python_out=./api/generated \
    --pyi_out=./api/generated \
    ./idl/crawler.proto

# ç”Ÿæˆ __init__.py
cat > api/generated/__init__.py << 'EOF'
"""è‡ªåŠ¨ç”Ÿæˆçš„ Protobuf ä»£ç """
from .crawler_pb2 import *
from .crawler_pb2_grpc import *

__all__ = [
    # è¯·æ±‚æ¶ˆæ¯
    "FzuNoticeRequest",
    "StartFzuCrawlerRequest",
    "ZhihuTopicRequest",
    "StartZhihuCrawlerRequest",
    "OsppProjectRequest",
    "StartOsppCrawlerRequest",
    "ExportToCsvRequest",
    
    # å“åº”æ¶ˆæ¯
    "FzuNoticeResponse",
    "ZhihuTopicResponse",
    "OsppProjectResponse",
    "ExportToCsvResponse",
    "CrawlerResult",
    
    # å®ä½“æ¶ˆæ¯
    "FzuNotice",
    "Attachment",
    "ZhihuQuestion",
    "ZhihuAnswer",
    "OsppProject",
    
    # æœåŠ¡
    "CrawlerServiceServicer",
    "CrawlerServiceStub",
]
EOF

echo "âœ… Protobuf ä»£ç ç”Ÿæˆå®Œæˆï¼"
echo "ğŸ“ è¾“å‡ºç›®å½•: api/generated/"
```

è¿è¡Œï¼š
```bash
chmod +x scripts/generate_proto.sh
./scripts/generate_proto.sh
```

### 3. ç”Ÿæˆçš„æ–‡ä»¶è¯´æ˜

ç”Ÿæˆåä¼šåœ¨ `api/generated/` ç›®å½•ä¸‹å¾—åˆ°ï¼š

```
api/generated/
â”œâ”€â”€ __init__.py              # æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ crawler_pb2.py           # Protobuf æ¶ˆæ¯ç±»
â”œâ”€â”€ crawler_pb2.pyi          # ç±»å‹æç¤ºæ–‡ä»¶
â””â”€â”€ crawler_pb2_grpc.py      # gRPC æœåŠ¡ç±»
```

### 4. ä½¿ç”¨ç”Ÿæˆçš„ä»£ç 

#### æ–¹å¼ä¸€ï¼šä½œä¸ºæ•°æ®æ¨¡å‹ä½¿ç”¨ï¼ˆæ¨èç”¨äº FastAPIï¼‰

```python
from api.generated import (
    FzuNoticeRequest,
    FzuNoticeResponse,
    StartFzuCrawlerRequest,
    CrawlerResult,
)

# åœ¨ FastAPI è·¯ç”±ä¸­ä½¿ç”¨ï¼ˆéœ€è¦è½¬æ¢ä¸º Pydanticï¼‰
from pydantic import BaseModel

class FzuNoticeRequestModel(BaseModel):
    page: int = 1
    page_size: int = 20
    keyword: str = None
    
    @staticmethod
    def to_proto():
        """è½¬æ¢ä¸º Protobuf æ¶ˆæ¯"""
        pass
```

#### æ–¹å¼äºŒï¼šä½œä¸º gRPC æœåŠ¡ä½¿ç”¨

```python
from api.generated import CrawlerServiceServicer
import grpc
from concurrent import futures

class CrawlerServiceImpl(CrawlerServiceServicer):
    """å®ç° gRPC æœåŠ¡"""
    
    async def StartFzuCrawler(self, request, context):
        # å®ç°é€»è¾‘
        pass
    
    async def GetFzuNotices(self, request, context):
        # å®ç°é€»è¾‘
        pass

# å¯åŠ¨ gRPC æœåŠ¡å™¨
server = grpc.aio.server(futures.ThreadPoolExecutor(max_workers=10))
crawler_pb2_grpc.add_CrawlerServiceServicer_to_server(
    CrawlerServiceImpl(), server
)
server.add_insecure_port('[::]:50051')
await server.start()
```

### 5. Protobuf è½¬ Pydanticï¼ˆç”¨äº FastAPIï¼‰

å¯ä»¥ä½¿ç”¨ `protobuf-to-pydantic` åº“ï¼š

```bash
pip install protobuf-to-pydantic
```

æˆ–æ‰‹åŠ¨è½¬æ¢ï¼š

```python
from pydantic import BaseModel
from typing import Optional, List

# æ‰‹åŠ¨å®šä¹‰ Pydantic æ¨¡å‹ï¼ˆä¸ Protobuf å¯¹åº”ï¼‰
class FzuNoticeRequest(BaseModel):
    page: int = 1
    page_size: int = 20
    keyword: Optional[str] = None

class Attachment(BaseModel):
    name: str
    download_count: int
    url: str

class FzuNotice(BaseModel):
    id: str
    publisher: str
    title: str
    date: str
    detail_url: str
    html_content: str
    attachments: List[Attachment] = []

class FzuNoticeResponse(BaseModel):
    notices: List[FzuNotice]
    total: int
    message: str
```

## Protocol Buffers æ¥å£å®šä¹‰è¯¦è§£ (idl/crawler.proto)

```protobuf
syntax = "proto3";

package crawler;

// ==================== ç¦å¤§æ•™åŠ¡é€šçŸ¥ ====================

message FzuNoticeRequest {
  int32 page = 1;           // é¡µç 
  int32 page_size = 2;      // æ¯é¡µæ•°é‡
  optional string keyword = 3;  // æœç´¢å…³é”®è¯
}

message FzuNotice {
  string id = 1;
  string publisher = 2;     // é€šçŸ¥äºº
  string title = 3;         // æ ‡é¢˜
  string date = 4;          // æ—¥æœŸ
  string detail_url = 5;    // è¯¦æƒ…é“¾æ¥
  string html_content = 6;  // è¯¦æƒ… HTML
  repeated Attachment attachments = 7;  // é™„ä»¶åˆ—è¡¨
}

message Attachment {
  string name = 1;          // é™„ä»¶å
  int32 download_count = 2; // ä¸‹è½½æ¬¡æ•°
  string url = 3;           // é™„ä»¶é“¾æ¥
}

message FzuNoticeResponse {
  repeated FzuNotice notices = 1;
  int32 total = 2;
  string message = 3;
}

message StartFzuCrawlerRequest {
  int32 target_count = 1;   // ç›®æ ‡çˆ¬å–æ•°é‡ (è‡³å°‘500)
}

message CrawlerResult {
  bool success = 1;
  string message = 2;
  int32 crawled_count = 3;
  int32 saved_count = 4;
}

// ==================== çŸ¥ä¹è¯é¢˜ ====================

message ZhihuTopicRequest {
  string topic_id = 1;      // è¯é¢˜ID
  int32 question_limit = 2; // é—®é¢˜æ•°é‡é™åˆ¶
  int32 answer_limit = 3;   // æ¯ä¸ªé—®é¢˜çš„å›ç­”æ•°é‡é™åˆ¶
}

message ZhihuQuestion {
  string id = 1;
  string title = 2;         // é—®é¢˜æ ‡é¢˜
  string content = 3;       // é—®é¢˜è¯¦ç»†å†…å®¹
  repeated ZhihuAnswer answers = 4;
}

message ZhihuAnswer {
  string id = 1;
  string content = 2;       // å›ç­”å†…å®¹ (çº¯æ–‡æœ¬)
  int32 vote_count = 3;     // èµåŒæ•°
  string author = 4;        // ä½œè€…
}

message ZhihuTopicResponse {
  repeated ZhihuQuestion questions = 1;
  string message = 2;
}

message StartZhihuCrawlerRequest {
  string topic_url = 1;
  int32 question_count = 2;
  int32 answer_per_question = 3;
}

// ==================== å¼€æºä¹‹å¤ ====================

message OsppProjectRequest {
  optional string keyword = 1;      // æœç´¢å…³é”®è¯
  optional string difficulty = 2;   // éš¾åº¦ç­›é€‰
  optional string tech_tag = 3;     // æŠ€æœ¯æ ‡ç­¾ç­›é€‰
}

message OsppProject {
  string id = 1;
  string name = 2;          // é¡¹ç›®å
  string difficulty = 3;    // éš¾åº¦
  repeated string tech_tags = 4;  // æŠ€æœ¯æ ‡ç­¾
  string description = 5;   // é¡¹ç›®ç®€è¿°
  string requirements = 6;  // äº§å‡ºè¦æ±‚
  optional string pdf_url = 7;    // ç”³è¯·ä¹¦PDFé“¾æ¥
}

message OsppProjectResponse {
  repeated OsppProject projects = 1;
  int32 total = 2;
  string message = 3;
}

message StartOsppCrawlerRequest {
  bool download_pdf = 1;    // æ˜¯å¦ä¸‹è½½PDF
}

// ==================== å¯¼å‡ºæ¥å£ ====================

message ExportToCsvRequest {
  string crawler_type = 1;  // "fzu" | "zhihu" | "ospp"
  optional string output_path = 2;
}

message ExportToCsvResponse {
  bool success = 1;
  string file_path = 2;
  string message = 3;
}

// ==================== æœåŠ¡å®šä¹‰ ====================

service CrawlerService {
  // ç¦å¤§æ•™åŠ¡é€šçŸ¥
  rpc StartFzuCrawler(StartFzuCrawlerRequest) returns (CrawlerResult);
  rpc GetFzuNotices(FzuNoticeRequest) returns (FzuNoticeResponse);
  
  // çŸ¥ä¹è¯é¢˜
  rpc StartZhihuCrawler(StartZhihuCrawlerRequest) returns (CrawlerResult);
  rpc GetZhihuTopics(ZhihuTopicRequest) returns (ZhihuTopicResponse);
  
  // å¼€æºä¹‹å¤
  rpc StartOsppCrawler(StartOsppCrawlerRequest) returns (CrawlerResult);
  rpc GetOsppProjects(OsppProjectRequest) returns (OsppProjectResponse);
  
  // é€šç”¨å¯¼å‡º
  rpc ExportToCsv(ExportToCsvRequest) returns (ExportToCsvResponse);
}
```

## æ•°æ®åº“æ¨¡å‹è®¾è®¡

### åŸºç¡€æ¨¡å‹ (models/base.py)

```python
from sqlalchemy import Column, Integer, DateTime
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func

Base = declarative_base()

class BaseModel(Base):
    """åŸºç¡€æ¨¡å‹ç±»"""
    __abstract__ = True
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
```

### ç¦å¤§é€šçŸ¥æ¨¡å‹ (models/fzu_notice.py)

```python
from sqlalchemy import Column, String, Text, Integer, ForeignKey
from sqlalchemy.orm import relationship
from models.base import BaseModel

class FzuNotice(BaseModel):
    __tablename__ = "fzu_notices"
    
    publisher = Column(String(100), nullable=False, comment="é€šçŸ¥äºº")
    title = Column(String(500), nullable=False, comment="æ ‡é¢˜")
    publish_date = Column(String(20), nullable=False, comment="å‘å¸ƒæ—¥æœŸ")
    detail_url = Column(String(500), nullable=False, unique=True, comment="è¯¦æƒ…é“¾æ¥")
    html_content = Column(Text, comment="è¯¦æƒ…HTMLå†…å®¹")
    
    # å…³è”é™„ä»¶
    attachments = relationship("FzuNoticeAttachment", back_populates="notice", 
                              cascade="all, delete-orphan")

class FzuNoticeAttachment(BaseModel):
    __tablename__ = "fzu_notice_attachments"
    
    notice_id = Column(Integer, ForeignKey("fzu_notices.id"), nullable=False)
    name = Column(String(500), nullable=False, comment="é™„ä»¶å")
    download_count = Column(Integer, default=0, comment="ä¸‹è½½æ¬¡æ•°")
    url = Column(String(500), nullable=False, comment="é™„ä»¶é“¾æ¥ç ")
    local_path = Column(String(1000), comment="æœ¬åœ°å­˜å‚¨è·¯å¾„")
    
    # å…³è”é€šçŸ¥
    notice = relationship("FzuNotice", back_populates="attachments")
```

### çŸ¥ä¹è¯é¢˜æ¨¡å‹ (models/zhihu_topic.py)

```python
from sqlalchemy import Column, String, Text, Integer, ForeignKey
from sqlalchemy.orm import relationship
from models.base import BaseModel

class ZhihuQuestion(BaseModel):
    __tablename__ = "zhihu_questions"
    
    question_id = Column(String(50), unique=True, nullable=False, comment="é—®é¢˜ID")
    topic_id = Column(String(50), nullable=False, comment="è¯é¢˜ID")
    title = Column(String(500), nullable=False, comment="é—®é¢˜æ ‡é¢˜")
    content = Column(Text, comment="é—®é¢˜è¯¦ç»†å†…å®¹")
    
    # å…³è”å›ç­”
    answers = relationship("ZhihuAnswer", back_populates="question",
                          cascade="all, delete-orphan")

class ZhihuAnswer(BaseModel):
    __tablename__ = "zhihu_answers"
    
    question_id = Column(Integer, ForeignKey("zhihu_questions.id"), nullable=False)
    answer_id = Column(String(50), unique=True, nullable=False, comment="å›ç­”ID")
    author = Column(String(200), comment="ä½œè€…")
    content = Column(Text, nullable=False, comment="å›ç­”å†…å®¹(çº¯æ–‡æœ¬)")
    vote_count = Column(Integer, default=0, comment="èµåŒæ•°")
    
    # å…³è”é—®é¢˜
    question = relationship("ZhihuQuestion", back_populates="answers")
```

### å¼€æºä¹‹å¤æ¨¡å‹ (models/ospp_project.py)

```python
from sqlalchemy import Column, String, Text, ARRAY
from models.base import BaseModel

class OsppProject(BaseModel):
    __tablename__ = "ospp_projects"
    
    project_id = Column(String(50), unique=True, nullable=False, comment="é¡¹ç›®ID")
    name = Column(String(500), nullable=False, comment="é¡¹ç›®å")
    difficulty = Column(String(50), comment="éš¾åº¦")
    tech_tags = Column(ARRAY(String), comment="æŠ€æœ¯æ ‡ç­¾åˆ—è¡¨")
    description = Column(Text, comment="é¡¹ç›®ç®€è¿°")
    requirements = Column(Text, comment="äº§å‡ºè¦æ±‚")
    pdf_url = Column(String(500), comment="ç”³è¯·ä¹¦PDFé“¾æ¥")
    pdf_local_path = Column(String(1000), comment="PDFæœ¬åœ°è·¯å¾„")
```

## API æ¥å£å®ç°ç¤ºä¾‹

### FastAPI è·¯ç”± (api/v1/fzu_notice.py)

```python
from fastapi import APIRouter, Depends, BackgroundTasks
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Optional
from api.deps import get_db
from schemas.fzu_notice import FzuNoticeRequest, FzuNoticeResponse, StartFzuCrawlerRequest
from crawlers.fzu_notice import FzuNoticeCrawler
from tasks.crawler_tasks import start_fzu_crawler_task

router = APIRouter(prefix="/fzu", tags=["ç¦å¤§æ•™åŠ¡é€šçŸ¥"])

@router.post("/start-crawler")
async def start_crawler(
    request: StartFzuCrawlerRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db)
):
    """å¯åŠ¨ç¦å¤§é€šçŸ¥çˆ¬è™« (å¼‚æ­¥ä»»åŠ¡)"""
    # ä½¿ç”¨ Celery å¼‚æ­¥ä»»åŠ¡
    task = start_fzu_crawler_task.delay(request.target_count)
    return {
        "task_id": task.id,
        "message": "Crawler task started"
    }

@router.get("/notices", response_model=FzuNoticeResponse)
async def get_notices(
    page: int = 1,
    page_size: int = 20,
    keyword: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
):
    """æŸ¥è¯¢ç¦å¤§é€šçŸ¥åˆ—è¡¨"""
    # å®ç°åˆ†é¡µæŸ¥è¯¢é€»è¾‘
    pass

@router.get("/notices/{notice_id}")
async def get_notice_detail(
    notice_id: int,
    db: AsyncSession = Depends(get_db)
):
    """è·å–é€šçŸ¥è¯¦æƒ…"""
    pass

@router.post("/export-csv")
async def export_to_csv(
    output_path: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
):
    """å¯¼å‡ºä¸º CSV"""
    pass
```

## å…·ä½“çˆ¬è™«å®ç°ç¤ºä¾‹

### ç¦å¤§æ•™åŠ¡é€šçŸ¥çˆ¬è™« (crawlers/fzu_notice.py)

```python
from typing import List, Dict, Any
from lxml import etree
from core.http_crawler import HttpCrawler
from models.fzu_notice import FzuNotice, FzuNoticeAttachment
from core.parser import Parser

class FzuNoticeCrawler(HttpCrawler):
    """ç¦å¤§æ•™åŠ¡é€šçŸ¥çˆ¬è™«"""
    
    BASE_URL = "https://jwch.fzu.edu.cn"
    LIST_URL = "https://jwch.fzu.edu.cn/jxtz.htm"
    
    async def fetch(self, target_count: int = 500, **kwargs) -> List[Dict[str, Any]]:
        """
        è·å–é€šçŸ¥åˆ—è¡¨
        
        å®ç°è¦ç‚¹:
        1. åˆ†æç½‘é¡µåˆ†é¡µè§„å¾‹ (jxtz.htm, jxtz/list2.htm, ...)
        2. å¾ªç¯è¯·æ±‚æ¯ä¸€é¡µ
        3. ä½¿ç”¨ XPath æå–é€šçŸ¥åˆ—è¡¨
        """
        notices = []
        page = 0
        
        while len(notices) < target_count:
            if page == 0:
                url = self.LIST_URL
            else:
                url = f"{self.BASE_URL}/jxtz/list{page + 1}.htm"
            
            response = await self.get(url)
            html = response.text
            
            # XPath æå–åˆ—è¡¨é¡¹
            tree = etree.HTML(html)
            items = tree.xpath('//ul[@class="news_list list-left"]/li')
            
            if not items:
                break
            
            for item in items:
                notice_data = {
                    'title': item.xpath('.//a/@title')[0] if item.xpath('.//a/@title') else '',
                    'detail_url': self.BASE_URL + item.xpath('.//a/@href')[0],
                    'date': item.xpath('.//span[@class="news_meta"]/text()')[0].strip(),
                    'publisher': self._extract_publisher(item.xpath('.//a/@title')[0])
                }
                notices.append(notice_data)
            
            page += 1
        
        return notices[:target_count]
    
    def _extract_publisher(self, title: str) -> str:
        """ä»æ ‡é¢˜ä¸­æå–å‘å¸ƒè€… (è´¨é‡åŠã€è®¡åˆ’ç§‘ç­‰)"""
        # ä½¿ç”¨æ­£åˆ™æˆ–å­—ç¬¦ä¸²å¤„ç†æå–
        import re
        match = re.search(r'ã€(.+?)ã€‘', title)
        return match.group(1) if match else "æœªçŸ¥"
    
    async def parse(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æå•æ¡é€šçŸ¥è¯¦æƒ…
        
        å®ç°è¦ç‚¹:
        1. è¯·æ±‚è¯¦æƒ…é¡µ
        2. æå–æ­£æ–‡HTML
        3. æå–é™„ä»¶ä¿¡æ¯ (åç§°ã€ä¸‹è½½æ¬¡æ•°ã€é“¾æ¥)
        4. æ¸…ç†æ–‡æœ¬
        """
        detail_url = raw_data['detail_url']
        response = await self.get(detail_url)
        html = response.text
        tree = etree.HTML(html)
        
        # æå–æ­£æ–‡
        content = tree.xpath('//div[@class="wp_articlecontent"]')[0]
        html_content = etree.tostring(content, encoding='unicode')
        
        # æå–é™„ä»¶
        attachments = []
        attachment_items = tree.xpath('//div[@class="wp_article_attach_list"]//li')
        for item in attachment_items:
            att = {
                'name': Parser.clean_text(item.xpath('.//a/text()')[0]),
                'url': item.xpath('.//a/@href')[0],
                'download_count': self._extract_download_count(item)
            }
            attachments.append(att)
        
        return {
            **raw_data,
            'html_content': html_content,
            'attachments': attachments
        }
    
    def _extract_download_count(self, item) -> int:
        """æå–ä¸‹è½½æ¬¡æ•°"""
        # ä»é¡µé¢ä¸­æå–ä¸‹è½½æ¬¡æ•°é€»è¾‘
        pass
    
    async def save(self, data: Dict[str, Any]) -> None:
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        notice = FzuNotice(
            publisher=data['publisher'],
            title=Parser.clean_text(data['title']),
            publish_date=data['date'],
            detail_url=data['detail_url'],
            html_content=data['html_content']
        )
        
        self.db_session.add(notice)
        await self.db_session.flush()  # è·å– notice.id
        
        # ä¿å­˜é™„ä»¶
        for att_data in data['attachments']:
            attachment = FzuNoticeAttachment(
                notice_id=notice.id,
                name=att_data['name'],
                download_count=att_data['download_count'],
                url=att_data['url']
            )
            self.db_session.add(attachment)
```

### çŸ¥ä¹è¯é¢˜çˆ¬è™« (crawlers/zhihu_topic.py)

```python
from typing import List, Dict, Any
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from core.selenium_crawler import SeleniumCrawler
from models.zhihu_topic import ZhihuQuestion, ZhihuAnswer
import time

class ZhihuTopicCrawler(SeleniumCrawler):
    """çŸ¥ä¹è¯é¢˜çˆ¬è™«"""
    
    async def fetch(self, topic_url: str, question_count: int = 20, **kwargs) -> List[Dict[str, Any]]:
        """
        è·å–è¯é¢˜ä¸‹çš„é—®é¢˜åˆ—è¡¨
        
        å®ç°è¦ç‚¹:
        1. ä½¿ç”¨ Selenium æ¨¡æ‹Ÿæµè§ˆå™¨
        2. å¤„ç†ç™»å½• (Cookie æˆ–æ‰«ç )
        3. æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
        4. æå–é—®é¢˜é“¾æ¥
        """
        self.driver.get(topic_url)
        time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
        
        # TODO: å¤„ç†ç™»å½• (å¯ä»¥æå‰ä¿å­˜ Cookie)
        
        questions = []
        scroll_count = 0
        max_scrolls = 10
        
        while len(questions) < question_count and scroll_count < max_scrolls:
            # æå–å½“å‰é¡µé¢çš„é—®é¢˜
            question_elements = self.driver.find_elements(By.CSS_SELECTOR, '.List-item')
            
            for elem in question_elements:
                if len(questions) >= question_count:
                    break
                
                try:
                    question_data = {
                        'title': elem.find_element(By.CSS_SELECTOR, 'h2').text,
                        'url': elem.find_element(By.CSS_SELECTOR, 'h2 a').get_attribute('href')
                    }
                    if question_data not in questions:
                        questions.append(question_data)
                except:
                    continue
            
            # æ»šåŠ¨åŠ è½½æ›´å¤š
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            scroll_count += 1
        
        return questions[:question_count]
    
    async def parse(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æé—®é¢˜è¯¦æƒ…å’Œå›ç­”
        
        å®ç°è¦ç‚¹:
        1. è®¿é—®é—®é¢˜é¡µé¢
        2. æå–é—®é¢˜è¯¦ç»†å†…å®¹
        3. æå–å›ç­”åˆ—è¡¨ (é™åˆ¶æ•°é‡)
        4. æå–å›ç­”çš„çº¯æ–‡æœ¬å†…å®¹
        """
        question_url = raw_data['url']
        self.driver.get(question_url)
        time.sleep(2)
        
        # æå–é—®é¢˜å†…å®¹
        try:
            content_elem = self.driver.find_element(By.CSS_SELECTOR, '.QuestionRichText')
            content = content_elem.text
        except:
            content = ""
        
        # æå–å›ç­”
        answers = []
        answer_elements = self.driver.find_elements(By.CSS_SELECTOR, '.List-item')[:10]
        
        for elem in answer_elements:
            try:
                answer_data = {
                    'author': elem.find_element(By.CSS_SELECTOR, '.AuthorInfo-name').text,
                    'content': elem.find_element(By.CSS_SELECTOR, '.RichContent-inner').text,
                    'vote_count': self._extract_vote_count(elem)
                }
                answers.append(answer_data)
            except:
                continue
        
        return {
            'title': raw_data['title'],
            'content': content,
            'answers': answers,
            'question_id': self._extract_question_id(question_url)
        }
    
    def _extract_vote_count(self, element) -> int:
        """æå–èµåŒæ•°"""
        try:
            vote_text = element.find_element(By.CSS_SELECTOR, '.VoteButton--up').text
            return int(vote_text) if vote_text.isdigit() else 0
        except:
            return 0
    
    def _extract_question_id(self, url: str) -> str:
        """ä»URLæå–é—®é¢˜ID"""
        import re
        match = re.search(r'/question/(\d+)', url)
        return match.group(1) if match else ""
    
    async def save(self, data: Dict[str, Any]) -> None:
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        question = ZhihuQuestion(
            question_id=data['question_id'],
            topic_id=data.get('topic_id', ''),
            title=data['title'],
            content=data['content']
        )
        
        self.db_session.add(question)
        await self.db_session.flush()
        
        # ä¿å­˜å›ç­”
        for ans_data in data['answers']:
            answer = ZhihuAnswer(
                question_id=question.id,
                answer_id=ans_data.get('answer_id', ''),
                author=ans_data['author'],
                content=ans_data['content'],
                vote_count=ans_data['vote_count']
            )
            self.db_session.add(answer)
```

### å¼€æºä¹‹å¤çˆ¬è™« (crawlers/ospp_project.py)

```python
from typing import List, Dict, Any
import json
from core.http_crawler import HttpCrawler
from models.ospp_project import OsppProject

class OsppProjectCrawler(HttpCrawler):
    """å¼€æºä¹‹å¤é¡¹ç›®çˆ¬è™«"""
    
    API_URL = "https://summer-ospp.ac.cn/api/projects"  # å‡è®¾çš„APIåœ°å€
    
    async def fetch(self, **kwargs) -> List[Dict[str, Any]]:
        """
        é€šè¿‡æŠ“åŒ…æ‰¾åˆ°çš„APIè·å–é¡¹ç›®åˆ—è¡¨
        
        å®ç°è¦ç‚¹:
        1. ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·æ‰¾åˆ°APIæ¥å£
        2. åˆ†æè¯·æ±‚å‚æ•° (åˆ†é¡µã€ç­›é€‰ç­‰)
        3. æ¨¡æ‹Ÿè¯·æ±‚è·å–JSONæ•°æ®
        """
        all_projects = []
        page = 1
        page_size = 50
        
        while True:
            params = {
                'page': page,
                'pageSize': page_size,
                'year': 2025
            }
            
            response = await self.get(self.API_URL, params=params)
            data = response.json()
            
            projects = data.get('data', {}).get('list', [])
            if not projects:
                break
            
            all_projects.extend(projects)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
            if len(projects) < page_size:
                break
            
            page += 1
        
        return all_projects
    
    async def parse(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æé¡¹ç›®è¯¦æƒ…
        
        å®ç°è¦ç‚¹:
        1. æå–é¡¹ç›®åŸºæœ¬ä¿¡æ¯
        2. å¦‚æœéœ€è¦æ›´è¯¦ç»†ä¿¡æ¯,è¯·æ±‚é¡¹ç›®è¯¦æƒ…API
        3. æå–PDFé“¾æ¥ (è¿›é˜¶)
        """
        project_id = raw_data.get('id')
        
        # è¯·æ±‚è¯¦æƒ…API (å¦‚æœæœ‰)
        detail_url = f"{self.API_URL}/{project_id}"
        response = await self.get(detail_url)
        detail_data = response.json().get('data', {})
        
        return {
            'project_id': str(project_id),
            'name': detail_data.get('name', ''),
            'difficulty': detail_data.get('difficulty', ''),
            'tech_tags': detail_data.get('techTags', []),
            'description': detail_data.get('description', ''),
            'requirements': detail_data.get('requirements', ''),
            'pdf_url': detail_data.get('pdfUrl', '')
        }
    
    async def save(self, data: Dict[str, Any]) -> None:
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        project = OsppProject(
            project_id=data['project_id'],
            name=data['name'],
            difficulty=data['difficulty'],
            tech_tags=data['tech_tags'],
            description=data['description'],
            requirements=data['requirements'],
            pdf_url=data['pdf_url']
        )
        
        self.db_session.add(project)
```

## é…ç½®æ–‡ä»¶

### åº”ç”¨é…ç½® (config/settings.py)

```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # åº”ç”¨é…ç½®
    APP_NAME: str = "Crawler Platform"
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = True
    
    # æ•°æ®åº“é…ç½®
    DATABASE_URL: str = "postgresql+asyncpg://user:pass@localhost:5432/crawler"
    
    # Redis é…ç½®
    REDIS_URL: str = "redis://localhost:6379/0"
    
    # Celery é…ç½®
    CELERY_BROKER_URL: str = "redis://localhost:6379/1"
    CELERY_RESULT_BACKEND: str = "redis://localhost:6379/2"
    
    # çˆ¬è™«é…ç½®
    REQUEST_TIMEOUT: int = 30
    MAX_RETRIES: int = 3
    DOWNLOAD_DIR: str = "./downloads"
    
    # Selenium é…ç½®
    SELENIUM_HEADLESS: bool = True
    CHROME_DRIVER_PATH: str = "/usr/local/bin/chromedriver"
    
    class Config:
        env_file = ".env"

@lru_cache()
def get_settings() -> Settings:
    return Settings()
```

### æ•°æ®åº“é…ç½® (config/database.py)

```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from config.settings import get_settings

settings = get_settings()

# åˆ›å»ºå¼‚æ­¥å¼•æ“
engine = create_async_engine(
    settings.DATABASE_URL,
    echo=settings.DEBUG,
    future=True,
    pool_pre_ping=True,
    pool_size=10,
    max_overflow=20
)

# åˆ›å»ºå¼‚æ­¥ä¼šè¯å·¥å‚
async_session_factory = sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autocommit=False,
    autoflush=False
)

async def get_db() -> AsyncSession:
    """ä¾èµ–æ³¨å…¥: è·å–æ•°æ®åº“ä¼šè¯"""
    async with async_session_factory() as session:
        try:
            yield session
        finally:
            await session.close()
```

## Celery ä»»åŠ¡é…ç½® (tasks/celery_app.py)

```python
from celery import Celery
from config.settings import get_settings

settings = get_settings()

celery_app = Celery(
    "crawler_tasks",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Asia/Shanghai',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=3600,  # 1å°æ—¶è¶…æ—¶
    task_soft_time_limit=3300,  # 55åˆ†é’Ÿè½¯è¶…æ—¶
)
```

## å¼€å‘æµç¨‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # macOS/Linux

# å®‰è£…é¡¹ç›®ä¾èµ–
pip install -r requirements.txt

# å®‰è£… sql-gen å·¥å…·ä¾èµ–
pip install -r pkg/sql-gen/requirements.txt

# å®‰è£… Protobuf å·¥å…·
pip install grpcio-tools

# å®‰è£… PostgreSQL (ä½¿ç”¨ Homebrew)
brew install postgresql@14
brew services start postgresql@14

# åˆ›å»ºæ•°æ®åº“
createdb crawler

# åˆ›å»ºæ•°æ®åº“ç”¨æˆ·
createuser -s go-mcp-demo
psql -c "ALTER USER \"go-mcp-demo\" WITH PASSWORD 'go-mcp-demo';"
```

### 2. é…ç½®æ–‡ä»¶è®¾ç½®

ç¼–è¾‘ `config/config.yaml`:

```yaml
pgsql:
  host: "127.0.0.1"
  port: 5432
  database: "crawler"
  user: "go-mcp-demo"
  password: "go-mcp-demo"

ai_provider:  # å¯é€‰ï¼Œç”¨äº AI åŠŸèƒ½
  model: "qwen3-vl-flash"
  remote:
    provider: "aliyun"
    base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
    api_key: "your-api-key"
```

### 3. ä»£ç ç”Ÿæˆå·¥ä½œæµ

#### æ­¥éª¤ 1: ç”Ÿæˆ Protobuf API ä»£ç 

```bash
# ä½¿ç”¨ Makefile
make proto

# æˆ–æ‰‹åŠ¨æ‰§è¡Œ
python -m grpc_tools.protoc \
    -I./idl \
    --python_out=./api/generated \
    --grpc_python_out=./api/generated \
    --pyi_out=./api/generated \
    ./idl/crawler.proto
```

#### æ­¥éª¤ 2: è®¾è®¡æ•°æ®åº“è¡¨ç»“æ„

åœ¨ PostgreSQL ä¸­åˆ›å»ºè¡¨ï¼š

```sql
-- ç¦å¤§æ•™åŠ¡é€šçŸ¥è¡¨
CREATE TABLE fzu_notices (
    id SERIAL PRIMARY KEY,
    publisher VARCHAR(100) NOT NULL,
    title VARCHAR(500) NOT NULL,
    publish_date VARCHAR(20) NOT NULL,
    detail_url VARCHAR(500) UNIQUE NOT NULL,
    html_content TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE
);

-- é™„ä»¶è¡¨
CREATE TABLE fzu_notice_attachments (
    id SERIAL PRIMARY KEY,
    notice_id INTEGER REFERENCES fzu_notices(id) ON DELETE CASCADE,
    name VARCHAR(500) NOT NULL,
    download_count INTEGER DEFAULT 0,
    url VARCHAR(500) NOT NULL,
    local_path VARCHAR(1000),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE
);

-- çŸ¥ä¹é—®é¢˜è¡¨
CREATE TABLE zhihu_questions (
    id SERIAL PRIMARY KEY,
    question_id VARCHAR(50) UNIQUE NOT NULL,
    topic_id VARCHAR(50) NOT NULL,
    title VARCHAR(500) NOT NULL,
    content TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE
);

-- çŸ¥ä¹å›ç­”è¡¨
CREATE TABLE zhihu_answers (
    id SERIAL PRIMARY KEY,
    question_id INTEGER REFERENCES zhihu_questions(id) ON DELETE CASCADE,
    answer_id VARCHAR(50) UNIQUE NOT NULL,
    author VARCHAR(200),
    content TEXT NOT NULL,
    vote_count INTEGER DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE
);

-- å¼€æºä¹‹å¤é¡¹ç›®è¡¨
CREATE TABLE ospp_projects (
    id SERIAL PRIMARY KEY,
    project_id VARCHAR(50) UNIQUE NOT NULL,
    name VARCHAR(500) NOT NULL,
    difficulty VARCHAR(50),
    tech_tags TEXT[],  -- PostgreSQL æ•°ç»„ç±»å‹
    description TEXT,
    requirements TEXT,
    pdf_url VARCHAR(500),
    pdf_local_path VARCHAR(1000),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE
);
```

#### æ­¥éª¤ 3: ç”Ÿæˆæ•°æ®åº“æ¨¡å‹

```bash
# ä½¿ç”¨ Makefile
make model

# æˆ–æ‰‹åŠ¨æ‰§è¡Œ
python pkg/sql-gen/main.py --config pkg/sql-gen/config.yaml

# ç”ŸæˆæŒ‡å®šè¡¨çš„æ¨¡å‹
python pkg/sql-gen/main.py -t fzu_notices fzu_notice_attachments

# åˆ—å‡ºæ‰€æœ‰è¡¨
python pkg/sql-gen/main.py --list-tables
```

ç”Ÿæˆçš„æ¨¡å‹ä¼šåœ¨ `pkg/sql-gen/generated_models/` ç›®å½•ï¼Œéœ€è¦æ‰‹åŠ¨å¤åˆ¶åˆ° `models/` ç›®å½•ï¼š

```bash
# å¤åˆ¶ç”Ÿæˆçš„æ¨¡å‹
cp pkg/sql-gen/generated_models/*.py models/
```

#### æ­¥éª¤ 4: ä¸€é”®ç”Ÿæˆæ‰€æœ‰ä»£ç 

```bash
make gen  # ç”Ÿæˆ API + æ¨¡å‹
```

### 4. å¼€å‘ä¸šåŠ¡é€»è¾‘

#### åˆ›å»ºçˆ¬è™«å®ç°

åœ¨ `internal/infra/external/` åˆ›å»ºå…·ä½“çš„çˆ¬è™«å®ç°ï¼š

```python
# internal/infra/external/fzu_crawler.py
from pkg.crawler.http_crawler import HttpCrawler

class FzuNoticeCrawler(HttpCrawler):
    async def fetch(self, **kwargs):
        # å®ç°çˆ¬å–é€»è¾‘
        pass
    
    async def parse(self, raw_data):
        # å®ç°è§£æé€»è¾‘
        pass
```

#### åˆ›å»ºä»“å‚¨å®ç°

åœ¨ `internal/infra/persistence/` å®ç°æ•°æ®æŒä¹…åŒ–ï¼š

```python
# internal/infra/persistence/fzu_repo_impl.py
from internal.domain.repository.fzu_repository import FzuRepository
from models.fzu_notices import FzuNotices

class FzuRepositoryImpl(FzuRepository):
    def __init__(self, db_session):
        self.db = db_session
    
    async def save(self, notice_data):
        # ä¿å­˜åˆ°æ•°æ®åº“
        pass
    
    async def find_all(self, page, page_size):
        # æŸ¥è¯¢æ•°æ®
        pass
```

#### åˆ›å»ºåº”ç”¨æœåŠ¡

åœ¨ `internal/application/` ç¼–æ’ä¸šåŠ¡æµç¨‹ï¼š

```python
# internal/application/fzu_service.py
class FzuNoticeService:
    def __init__(self, repository, crawler):
        self.repository = repository
        self.crawler = crawler
    
    async def start_crawl(self, target_count):
        # 1. è°ƒç”¨çˆ¬è™«è·å–æ•°æ®
        data = await self.crawler.run(target_count=target_count)
        
        # 2. ä¿å­˜åˆ°æ•°æ®åº“
        for item in data:
            await self.repository.save(item)
        
        return {"success": True, "count": len(data)}
    
    async def get_notices(self, page, page_size):
        return await self.repository.find_all(page, page_size)
```

### 5. æµ‹è¯•è¿è¡Œ

```bash
# æµ‹è¯•çˆ¬è™«
python scripts/run_crawler.py --crawler fzu --count 500

# æµ‹è¯•æ•°æ®åº“è¿æ¥
python -c "from models import Base; print('Models loaded successfully')"

# æµ‹è¯• Protobuf
python -c "from api.generated import FzuNoticeRequest; print('Protobuf loaded')"
```

## ä¾èµ–æ¸…å•

### ä¸»é¡¹ç›®ä¾èµ– (requirements.txt)

```txt
# æ•°æ®åº“
sqlalchemy==2.0.23
psycopg2-binary==2.9.9

# çˆ¬è™«ç›¸å…³
httpx==0.25.2
lxml==4.9.3
selenium==4.15.2

# é…ç½®
pyyaml==6.0.1

# Protobuf
protobuf==4.25.1
grpcio==1.60.0
grpcio-tools==1.60.0

# Web æ¡†æ¶ (å¯é€‰)
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0

# æµ‹è¯• (å¯é€‰)
pytest==7.4.3
pytest-asyncio==0.21.1
```

### sql-gen å·¥å…·ä¾èµ– (pkg/sql-gen/requirements.txt)

```txt
sqlalchemy==2.0.23
asyncpg==0.29.0
psycopg2-binary==2.9.9
pyyaml==6.0.1
typing-extensions==4.9.0
```

## å¼€å‘å»ºè®®

### 1. ä»£ç ç»„ç»‡
- ä¿æŒçˆ¬è™«é€»è¾‘ç‹¬ç«‹,ä¾¿äºå¤ç”¨
- ä½¿ç”¨ä¾èµ–æ³¨å…¥ç®¡ç†æ•°æ®åº“è¿æ¥
- éµå¾ªå•ä¸€èŒè´£åŸåˆ™

### 2. é”™è¯¯å¤„ç†
- ç½‘ç»œè¯·æ±‚æ·»åŠ é‡è¯•æœºåˆ¶
- è§£æå¤±è´¥æ—¶è®°å½•æ—¥å¿—ä½†ä¸ä¸­æ–­æµç¨‹
- ä½¿ç”¨ try-except åŒ…è£¹å…³é”®ä»£ç 

### 3. æ€§èƒ½ä¼˜åŒ–
- ä½¿ç”¨å¼‚æ­¥ I/O (async/await)
- åˆç†ä½¿ç”¨è¿æ¥æ± 
- é¿å…åœ¨å¾ªç¯ä¸­è¿›è¡Œæ•°æ®åº“æ“ä½œ

### 4. æ•°æ®æ¸…æ´—
- ç»Ÿä¸€çš„æ–‡æœ¬æ¸…æ´—å‡½æ•°
- å»é™¤ HTML æ ‡ç­¾
- å¤„ç†ç‰¹æ®Šå­—ç¬¦

### 5. åçˆ¬åº”å¯¹
- è®¾ç½®åˆç†çš„è¯·æ±‚é—´éš”
- ä½¿ç”¨ä»£ç†æ±  (å¦‚éœ€è¦)
- è½®æ¢ User-Agent
- å¤„ç† Cookie å’Œ Session

### 6. æ—¥å¿—è®°å½•
- ä½¿ç”¨ loguru è®°å½•è¯¦ç»†æ—¥å¿—
- åŒºåˆ†ä¸åŒçº§åˆ« (DEBUG, INFO, ERROR)
- è®°å½•å…³é”®æ“ä½œå’Œå¼‚å¸¸

## CSV å¯¼å‡ºç¤ºä¾‹ (utils/csv_writer.py)

```python
import csv
from typing import List, Dict, Any
from pathlib import Path

class CsvWriter:
    """CSV å¯¼å‡ºå·¥å…·"""
    
    @staticmethod
    async def write_fzu_notices(notices: List[Dict[str, Any]], output_path: str):
        """å¯¼å‡ºç¦å¤§é€šçŸ¥åˆ° CSV"""
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['ID', 'é€šçŸ¥äºº', 'æ ‡é¢˜', 'æ—¥æœŸ', 'è¯¦æƒ…é“¾æ¥', 'é™„ä»¶æ•°é‡']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            
            writer.writeheader()
            for notice in notices:
                writer.writerow({
                    'ID': notice['id'],
                    'é€šçŸ¥äºº': notice['publisher'],
                    'æ ‡é¢˜': notice['title'],
                    'æ—¥æœŸ': notice['publish_date'],
                    'è¯¦æƒ…é“¾æ¥': notice['detail_url'],
                    'é™„ä»¶æ•°é‡': len(notice.get('attachments', []))
                })
    
    @staticmethod
    async def write_zhihu_topics(questions: List[Dict[str, Any]], output_path: str):
        """å¯¼å‡ºçŸ¥ä¹è¯é¢˜åˆ° CSV"""
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['é—®é¢˜ID', 'é—®é¢˜æ ‡é¢˜', 'é—®é¢˜å†…å®¹', 'å›ç­”æ•°', 'å›ç­”å†…å®¹']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            
            writer.writeheader()
            for question in questions:
                for answer in question.get('answers', []):
                    writer.writerow({
                        'é—®é¢˜ID': question['question_id'],
                        'é—®é¢˜æ ‡é¢˜': question['title'],
                        'é—®é¢˜å†…å®¹': question['content'],
                        'å›ç­”æ•°': len(question.get('answers', [])),
                        'å›ç­”å†…å®¹': answer['content']
                    })
```

## å¿«é€Ÿå¼€å§‹æ¸…å•

### æ–°é¡¹ç›®åˆå§‹åŒ–

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <your-repo>
cd crawler

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt
pip install -r pkg/sql-gen/requirements.txt

# 4. é…ç½®æ•°æ®åº“
createdb crawler
# ç¼–è¾‘ config/config.yaml

# 5. åˆ›å»ºæ•°æ®åº“è¡¨
psql -d crawler -f schema.sql

# 6. ç”Ÿæˆä»£ç 
make gen  # ç”Ÿæˆ API + æ¨¡å‹

# 7. å¼€å§‹å¼€å‘
# - åœ¨ internal/infra/external/ å®ç°çˆ¬è™«
# - åœ¨ internal/infra/persistence/ å®ç°ä»“å‚¨
# - åœ¨ internal/application/ å®ç°åº”ç”¨æœåŠ¡
```

### å¸¸ç”¨å‘½ä»¤

```bash
# ä»£ç ç”Ÿæˆ
make proto          # ç”Ÿæˆ Protobuf API
make model          # ç”Ÿæˆæ•°æ®åº“æ¨¡å‹
make gen            # ç”Ÿæˆæ‰€æœ‰ä»£ç 

# æŸ¥çœ‹æ•°æ®åº“è¡¨
make model --list-tables

# è¿è¡Œæµ‹è¯•
pytest tests/

# ä»£ç æ ¼å¼åŒ–
black .
isort .
```

## æ¶æ„ä¼˜åŠ¿

### 1. **æ¸…æ™°çš„åˆ†å±‚ç»“æ„**
- API å±‚ï¼šæ¥å£å®šä¹‰
- åº”ç”¨å±‚ï¼šä¸šåŠ¡ç¼–æ’
- é¢†åŸŸå±‚ï¼šæ ¸å¿ƒä¸šåŠ¡
- åŸºç¡€è®¾æ–½å±‚ï¼šæŠ€æœ¯å®ç°

### 2. **é«˜åº¦å¯æµ‹è¯•**
- æ¯å±‚ç‹¬ç«‹æµ‹è¯•
- æ¥å£ä¾èµ–æ³¨å…¥
- Mock å‹å¥½

### 3. **æ˜“äºç»´æŠ¤**
- å…³æ³¨ç‚¹åˆ†ç¦»
- ä½è€¦åˆé«˜å†…èš
- ä¸šåŠ¡é€»è¾‘ä¸æŠ€æœ¯å®ç°è§£è€¦

### 4. **æ‰©å±•æ€§å¼º**
- æ–°å¢çˆ¬è™«ï¼šç»§æ‰¿ `BaseCrawler`
- æ–°å¢æ•°æ®æºï¼šå®ç° `Repository` æ¥å£
- æ–°å¢ä¸šåŠ¡ï¼šåœ¨åº”ç”¨å±‚ç¼–æ’

### 5. **ä»£ç ç”Ÿæˆ**
- IDL â†’ API ä»£ç ï¼ˆProtobufï¼‰
- æ•°æ®åº“ â†’ æ¨¡å‹ä»£ç ï¼ˆsql-genï¼‰
- å‡å°‘é‡å¤åŠ³åŠ¨

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆè¦åˆ†è¿™ä¹ˆå¤šå±‚ï¼Ÿ
**A**: éµå¾ª Clean Architecture åŸåˆ™ï¼Œä¿è¯ï¼š
- ä¸šåŠ¡é€»è¾‘ä¸ä¾èµ–æ¡†æ¶
- æ˜“äºæµ‹è¯•
- æŠ€æœ¯æ ˆå¯æ›¿æ¢

### Q2: çˆ¬è™«ä¸ºä»€ä¹ˆä¸ç›´æ¥ä¿å­˜æ•°æ®ï¼Ÿ
**A**: éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼š
- çˆ¬è™«åªè´Ÿè´£æ•°æ®è·å–å’Œè§£æ
- ä»“å‚¨è´Ÿè´£æ•°æ®æŒä¹…åŒ–
- ä¾¿äºå•å…ƒæµ‹è¯•å’Œå¤ç”¨

### Q3: ä¸ºä»€ä¹ˆä½¿ç”¨ Protobufï¼Ÿ
**A**: 
- å¼ºç±»å‹çº¦æŸ
- è·¨è¯­è¨€æ”¯æŒ
- æ¥å£å³æ–‡æ¡£
- ä¾¿äºå‰åç«¯åä½œ

### Q4: sql-gen ç”Ÿæˆçš„æ¨¡å‹å¦‚ä½•ä½¿ç”¨ï¼Ÿ
**A**: 
```bash
# ç”Ÿæˆæ¨¡å‹
make model

# å¤åˆ¶åˆ°é¡¹ç›®
cp pkg/sql-gen/generated_models/*.py models/

# åœ¨ä»£ç ä¸­ä½¿ç”¨
from models.fzu_notices import FzuNotices
```

## è¿›é˜¶æ‰©å±•

### 1. æ·»åŠ ç¼“å­˜å±‚
åœ¨ `internal/infra/` æ·»åŠ  Redis ç¼“å­˜å®ç°

### 2. æ·»åŠ æ¶ˆæ¯é˜Ÿåˆ—
ä½¿ç”¨ Celery å¤„ç†å¼‚æ­¥çˆ¬è™«ä»»åŠ¡

### 3. æ·»åŠ ç›‘æ§
- Prometheus + Grafana
- çˆ¬è™«ä»»åŠ¡ç›‘æ§
- æ€§èƒ½æŒ‡æ ‡æ”¶é›†

### 4. å®¹å™¨åŒ–éƒ¨ç½²
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main.py"]
```

## å‚è€ƒèµ„æ–™

- [Clean Architecture](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)
- [Domain-Driven Design](https://martinfowler.com/bliki/DomainDrivenDesign.html)
- [Protocol Buffers](https://protobuf.dev/)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)

## æ€»ç»“

è¿™ä¸ªé¡¹ç›®æ¶æ„å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **DDD + Clean Architecture**ï¼šæ¸…æ™°çš„åˆ†å±‚å’ŒèŒè´£åˆ’åˆ†
2. **ä»£ç ç”Ÿæˆé©±åŠ¨**ï¼šIDL â†’ APIï¼Œæ•°æ®åº“ â†’ æ¨¡å‹
3. **é«˜åº¦æ¨¡å—åŒ–**ï¼šçˆ¬è™«å¼•æ“ã€æ¨¡å‹ç”Ÿæˆå™¨ç‹¬ç«‹å¯å¤ç”¨
4. **æ˜“äºæ‰©å±•**ï¼šæ–°å¢åŠŸèƒ½åªéœ€å®ç°æ¥å£
5. **ç”Ÿäº§å°±ç»ª**ï¼šå®Œæ•´çš„é”™è¯¯å¤„ç†ã€æ—¥å¿—ã€æµ‹è¯•æ”¯æŒ

## ä¸‹ä¸€æ­¥

1. âœ… å®Œæˆç¯å¢ƒæ­å»ºå’Œé…ç½®
2. âœ… ç”Ÿæˆ API å’Œæ¨¡å‹ä»£ç 
3. ğŸ”² å®ç°ç¦å¤§æ•™åŠ¡é€šçŸ¥çˆ¬è™«
4. ğŸ”² å®ç°çŸ¥ä¹è¯é¢˜çˆ¬è™«
5. ğŸ”² å®ç°å¼€æºä¹‹å¤çˆ¬è™«
6. ğŸ”² ç¼–å†™å•å…ƒæµ‹è¯•
7. ğŸ”² æ·»åŠ  FastAPI HTTP æœåŠ¡ï¼ˆå¯é€‰ï¼‰
8. ğŸ”² å®¹å™¨åŒ–éƒ¨ç½²ï¼ˆå¯é€‰ï¼‰

æœ‰é—®é¢˜éšæ—¶æé—®ï¼ğŸš€
