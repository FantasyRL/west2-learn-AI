# çˆ¬è™«é¡¹ç›®å¼€å‘æŒ‡å—

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäº Python çš„é€šç”¨çˆ¬è™«å¹³å°,æ”¯æŒå¤šç§çˆ¬è™«ä»»åŠ¡çš„ç»Ÿä¸€ç®¡ç†å’Œè°ƒåº¦ã€‚é‡‡ç”¨å‰åç«¯åˆ†ç¦»æ¶æ„,é€šè¿‡ HTTP API æä¾›æœåŠ¡,æ•°æ®æŒä¹…åŒ–åˆ° PostgreSQL æ•°æ®åº“ã€‚

### æ ¸å¿ƒç‰¹æ€§

- **å¯å¤ç”¨çš„çˆ¬è™«æ ¸å¿ƒå¼•æ“**:æŠ½è±¡å‡ºé€šç”¨çš„çˆ¬è™«ç»„ä»¶,æ”¯æŒå¿«é€Ÿæ‰©å±•æ–°çš„çˆ¬è™«ä»»åŠ¡
- **ç»Ÿä¸€çš„ API æ¥å£**:é€šè¿‡ Protocol Buffers å®šä¹‰æ¥å£è§„èŒƒ
- **æ•°æ®æŒä¹…åŒ–**:ä½¿ç”¨ PostgreSQL + SQLAlchemy ORM
- **ä»»åŠ¡è°ƒåº¦**:æ”¯æŒå¼‚æ­¥ä»»åŠ¡å’Œå®šæ—¶ä»»åŠ¡
- **çµæ´»é…ç½®**:æ”¯æŒå¤šç§çˆ¬è™«ç­–ç•¥(requestsã€Seleniumã€API è°ƒç”¨)

## æŠ€æœ¯é€‰å‹

### 1. Web æ¡†æ¶
**FastAPI** - ç°ä»£åŒ–çš„ Python Web æ¡†æ¶

**é€‰å‹ç†ç”±**:
- åŸç”Ÿæ”¯æŒ async/await,æ€§èƒ½ä¼˜å¼‚
- è‡ªåŠ¨ç”Ÿæˆ OpenAPI æ–‡æ¡£
- ç±»å‹æç¤ºæ”¯æŒ,å¼€å‘ä½“éªŒå¥½
- ä¸ Pydantic æ·±åº¦é›†æˆ

### 2. ORM æ¡†æ¶
**SQLAlchemy 2.0 + Alembic**

**é€‰å‹ç†ç”±**:
- SQLAlchemy æ˜¯ Python ç”Ÿæ€æœ€æˆç†Ÿçš„ ORM
- 2.0 ç‰ˆæœ¬æ”¯æŒç±»å‹æç¤º,ç±»ä¼¼ GORM çš„é“¾å¼è°ƒç”¨
- Alembic æä¾›æ•°æ®åº“è¿ç§»åŠŸèƒ½
- æ”¯æŒå¼‚æ­¥æ“ä½œ (sqlalchemy.ext.asyncio)

**ä»£ç ç”Ÿæˆå·¥å…·**: sqlacodegen (ç±»ä¼¼ gorm-gen)
```bash
sqlacodegen postgresql://user:pass@localhost/dbname --outfile models.py
```

### 3. IDL å®šä¹‰
**Protocol Buffers (protobuf)** + **grpcio-tools**

**é€‰å‹ç†ç”±**:
- è·¨è¯­è¨€æ”¯æŒ
- å¼ºç±»å‹çº¦æŸ
- è‡ªåŠ¨ç”Ÿæˆä»£ç 
- FastAPI å¯ä»¥é€šè¿‡ protobuf-to-pydantic è½¬æ¢ä¸º Pydantic æ¨¡å‹

### 4. çˆ¬è™«ç›¸å…³åº“
- **requests**: HTTP è¯·æ±‚
- **selenium**: æµè§ˆå™¨è‡ªåŠ¨åŒ–
- **lxml**: XPath è§£æ
- **BeautifulSoup4**: HTML è§£æ (å¤‡é€‰)
- **httpx**: å¼‚æ­¥ HTTP å®¢æˆ·ç«¯

### 5. ä»»åŠ¡è°ƒåº¦
**Celery** + **Redis**

**é€‰å‹ç†ç”±**:
- æˆç†Ÿçš„åˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—
- æ”¯æŒå®šæ—¶ä»»åŠ¡
- æ”¯æŒä»»åŠ¡é‡è¯•å’Œå¤±è´¥å¤„ç†

## é¡¹ç›®ç»“æ„

```
crawler/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ guide.md                    # æœ¬æ–‡æ¡£
â”œâ”€â”€ requirements.txt            # ä¾èµ–åŒ…
â”œâ”€â”€ config/                     # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py            # åº”ç”¨é…ç½®
â”‚   â””â”€â”€ database.py            # æ•°æ®åº“é…ç½®
â”œâ”€â”€ proto/                      # Protocol Buffers å®šä¹‰
â”‚   â”œâ”€â”€ crawler.proto          # çˆ¬è™«æœåŠ¡æ¥å£å®šä¹‰
â”‚   â””â”€â”€ generated/             # è‡ªåŠ¨ç”Ÿæˆçš„ Python ä»£ç 
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ crawler_pb2.py
â”œâ”€â”€ models/                     # æ•°æ®åº“æ¨¡å‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                # åŸºç¡€æ¨¡å‹ç±»
â”‚   â”œâ”€â”€ fzu_notice.py          # ç¦å¤§é€šçŸ¥æ¨¡å‹
â”‚   â”œâ”€â”€ zhihu_topic.py         # çŸ¥ä¹è¯é¢˜æ¨¡å‹
â”‚   â””â”€â”€ ospp_project.py        # å¼€æºä¹‹å¤é¡¹ç›®æ¨¡å‹
â”œâ”€â”€ schemas/                    # Pydantic æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fzu_notice.py
â”‚   â”œâ”€â”€ zhihu_topic.py
â”‚   â””â”€â”€ ospp_project.py
â”œâ”€â”€ core/                       # æ ¸å¿ƒçˆ¬è™«å¼•æ“
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_crawler.py        # æŠ½è±¡åŸºç±»
â”‚   â”œâ”€â”€ http_crawler.py        # HTTP çˆ¬è™«åŸºç±»
â”‚   â”œâ”€â”€ selenium_crawler.py    # Selenium çˆ¬è™«åŸºç±»
â”‚   â”œâ”€â”€ parser.py              # æ•°æ®è§£æå™¨
â”‚   â”œâ”€â”€ downloader.py          # æ–‡ä»¶ä¸‹è½½å™¨
â”‚   â””â”€â”€ middleware.py          # ä¸­é—´ä»¶ (è¯·æ±‚é‡è¯•ã€æ—¥å¿—ç­‰)
â”œâ”€â”€ crawlers/                   # å…·ä½“çˆ¬è™«å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fzu_notice.py          # ä½œä¸š1: ç¦å¤§æ•™åŠ¡é€šçŸ¥
â”‚   â”œâ”€â”€ zhihu_topic.py         # ä½œä¸š2: çŸ¥ä¹è¯é¢˜
â”‚   â””â”€â”€ ospp_project.py        # ä½œä¸š3: å¼€æºä¹‹å¤
â”œâ”€â”€ api/                        # API æ¥å£
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ deps.py                # ä¾èµ–æ³¨å…¥
â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ fzu_notice.py      # ç¦å¤§é€šçŸ¥æ¥å£
â”‚   â”‚   â”œâ”€â”€ zhihu_topic.py     # çŸ¥ä¹è¯é¢˜æ¥å£
â”‚   â”‚   â””â”€â”€ ospp_project.py    # å¼€æºä¹‹å¤æ¥å£
â”‚   â””â”€â”€ router.py              # è·¯ç”±æ±‡æ€»
â”œâ”€â”€ tasks/                      # Celery ä»»åŠ¡
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ celery_app.py          # Celery é…ç½®
â”‚   â””â”€â”€ crawler_tasks.py       # çˆ¬è™«å¼‚æ­¥ä»»åŠ¡
â”œâ”€â”€ utils/                      # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py              # æ—¥å¿—é…ç½®
â”‚   â”œâ”€â”€ csv_writer.py          # CSV å¯¼å‡º
â”‚   â””â”€â”€ validators.py          # æ•°æ®éªŒè¯
â”œâ”€â”€ migrations/                 # æ•°æ®åº“è¿ç§»
â”‚   â””â”€â”€ versions/
â”œâ”€â”€ tests/                      # æµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_crawlers/
â”‚   â””â”€â”€ test_api/
â”œâ”€â”€ main.py                     # FastAPI åº”ç”¨å…¥å£
â”œâ”€â”€ alembic.ini                # Alembic é…ç½®
â””â”€â”€ README.md                   # é¡¹ç›®è¯´æ˜
```

## æ ¸å¿ƒè®¾è®¡

### 1. çˆ¬è™«åŸºç±»è®¾è®¡ (core/base_crawler.py)

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession

class BaseCrawler(ABC):
    """çˆ¬è™«æŠ½è±¡åŸºç±»"""
    
    def __init__(self, db_session: AsyncSession):
        self.db_session = db_session
        self.logger = get_logger(self.__class__.__name__)
    
    @abstractmethod
    async def fetch(self, **kwargs) -> List[Dict[str, Any]]:
        """
        è·å–åŸå§‹æ•°æ®
        è¿”å›: åŸå§‹æ•°æ®åˆ—è¡¨
        """
        pass
    
    @abstractmethod
    async def parse(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æå•æ¡æ•°æ®
        è¿”å›: è§£æåçš„ç»“æ„åŒ–æ•°æ®
        """
        pass
    
    @abstractmethod
    async def save(self, data: Dict[str, Any]) -> None:
        """
        ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
        """
        pass
    
    async def run(self, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„çˆ¬è™«æµç¨‹
        è¿”å›: æ‰§è¡Œç»“æœç»Ÿè®¡
        """
        try:
            # 1. è·å–æ•°æ®
            raw_data_list = await self.fetch(**kwargs)
            self.logger.info(f"Fetched {len(raw_data_list)} items")
            
            # 2. è§£ææ•°æ®
            parsed_data_list = []
            for raw_data in raw_data_list:
                try:
                    parsed = await self.parse(raw_data)
                    parsed_data_list.append(parsed)
                except Exception as e:
                    self.logger.error(f"Parse error: {e}")
                    continue
            
            # 3. ä¿å­˜æ•°æ®
            success_count = 0
            for data in parsed_data_list:
                try:
                    await self.save(data)
                    success_count += 1
                except Exception as e:
                    self.logger.error(f"Save error: {e}")
                    continue
            
            await self.db_session.commit()
            
            return {
                "total": len(raw_data_list),
                "parsed": len(parsed_data_list),
                "saved": success_count
            }
        except Exception as e:
            await self.db_session.rollback()
            self.logger.error(f"Crawler run error: {e}")
            raise
```

### 2. HTTP çˆ¬è™«åŸºç±» (core/http_crawler.py)

```python
from typing import Optional
import httpx
from core.base_crawler import BaseCrawler

class HttpCrawler(BaseCrawler):
    """HTTP çˆ¬è™«åŸºç±»"""
    
    def __init__(self, db_session: AsyncSession, 
                 headers: Optional[Dict[str, str]] = None,
                 timeout: int = 30):
        super().__init__(db_session)
        self.headers = headers or self._default_headers()
        self.timeout = timeout
        self.client = None
    
    def _default_headers(self) -> Dict[str, str]:
        return {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"
        }
    
    async def __aenter__(self):
        self.client = httpx.AsyncClient(
            headers=self.headers,
            timeout=self.timeout,
            follow_redirects=True
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.client:
            await self.client.aclose()
    
    async def get(self, url: str, **kwargs) -> httpx.Response:
        """å‘é€ GET è¯·æ±‚"""
        if not self.client:
            raise RuntimeError("Client not initialized. Use 'async with'")
        return await self.client.get(url, **kwargs)
    
    async def post(self, url: str, **kwargs) -> httpx.Response:
        """å‘é€ POST è¯·æ±‚"""
        if not self.client:
            raise RuntimeError("Client not initialized. Use 'async with'")
        return await self.client.post(url, **kwargs)
```

### 3. Selenium çˆ¬è™«åŸºç±» (core/selenium_crawler.py)

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from core.base_crawler import BaseCrawler

class SeleniumCrawler(BaseCrawler):
    """Selenium çˆ¬è™«åŸºç±»"""
    
    def __init__(self, db_session: AsyncSession, headless: bool = True):
        super().__init__(db_session)
        self.headless = headless
        self.driver = None
    
    def _init_driver(self):
        """åˆå§‹åŒ– Chrome é©±åŠ¨"""
        options = Options()
        if self.headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
    
    def __enter__(self):
        self._init_driver()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.driver:
            self.driver.quit()
    
    def get_page(self, url: str):
        """è®¿é—®é¡µé¢"""
        if not self.driver:
            raise RuntimeError("Driver not initialized. Use 'with'")
        self.driver.get(url)
```

### 4. æ•°æ®è§£æå™¨ (core/parser.py)

```python
from lxml import etree
from typing import List, Union

class Parser:
    """æ•°æ®è§£æå·¥å…·ç±»"""
    
    @staticmethod
    def xpath(html: str, xpath_expr: str) -> List[str]:
        """XPath è§£æ"""
        tree = etree.HTML(html)
        return tree.xpath(xpath_expr)
    
    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ (å»é™¤ç©ºç™½ã€æ¢è¡Œç­‰)"""
        return ' '.join(text.split()).strip()
    
    @staticmethod
    def extract_links(html: str, base_url: str = "") -> List[str]:
        """æå–æ‰€æœ‰é“¾æ¥"""
        tree = etree.HTML(html)
        links = tree.xpath('//a/@href')
        if base_url:
            from urllib.parse import urljoin
            links = [urljoin(base_url, link) for link in links]
        return links
```

## Protocol Buffers æ¥å£å®šä¹‰ (proto/crawler.proto)

```protobuf
syntax = "proto3";

package crawler;

// ==================== ç¦å¤§æ•™åŠ¡é€šçŸ¥ ====================

message FzuNoticeRequest {
  int32 page = 1;           // é¡µç 
  int32 page_size = 2;      // æ¯é¡µæ•°é‡
  optional string keyword = 3;  // æœç´¢å…³é”®è¯
}

message FzuNotice {
  string id = 1;
  string publisher = 2;     // é€šçŸ¥äºº
  string title = 3;         // æ ‡é¢˜
  string date = 4;          // æ—¥æœŸ
  string detail_url = 5;    // è¯¦æƒ…é“¾æ¥
  string html_content = 6;  // è¯¦æƒ… HTML
  repeated Attachment attachments = 7;  // é™„ä»¶åˆ—è¡¨
}

message Attachment {
  string name = 1;          // é™„ä»¶å
  int32 download_count = 2; // ä¸‹è½½æ¬¡æ•°
  string url = 3;           // é™„ä»¶é“¾æ¥
}

message FzuNoticeResponse {
  repeated FzuNotice notices = 1;
  int32 total = 2;
  string message = 3;
}

message StartFzuCrawlerRequest {
  int32 target_count = 1;   // ç›®æ ‡çˆ¬å–æ•°é‡ (è‡³å°‘500)
}

message CrawlerResult {
  bool success = 1;
  string message = 2;
  int32 crawled_count = 3;
  int32 saved_count = 4;
}

// ==================== çŸ¥ä¹è¯é¢˜ ====================

message ZhihuTopicRequest {
  string topic_id = 1;      // è¯é¢˜ID
  int32 question_limit = 2; // é—®é¢˜æ•°é‡é™åˆ¶
  int32 answer_limit = 3;   // æ¯ä¸ªé—®é¢˜çš„å›ç­”æ•°é‡é™åˆ¶
}

message ZhihuQuestion {
  string id = 1;
  string title = 2;         // é—®é¢˜æ ‡é¢˜
  string content = 3;       // é—®é¢˜è¯¦ç»†å†…å®¹
  repeated ZhihuAnswer answers = 4;
}

message ZhihuAnswer {
  string id = 1;
  string content = 2;       // å›ç­”å†…å®¹ (çº¯æ–‡æœ¬)
  int32 vote_count = 3;     // èµåŒæ•°
  string author = 4;        // ä½œè€…
}

message ZhihuTopicResponse {
  repeated ZhihuQuestion questions = 1;
  string message = 2;
}

message StartZhihuCrawlerRequest {
  string topic_url = 1;
  int32 question_count = 2;
  int32 answer_per_question = 3;
}

// ==================== å¼€æºä¹‹å¤ ====================

message OsppProjectRequest {
  optional string keyword = 1;      // æœç´¢å…³é”®è¯
  optional string difficulty = 2;   // éš¾åº¦ç­›é€‰
  optional string tech_tag = 3;     // æŠ€æœ¯æ ‡ç­¾ç­›é€‰
}

message OsppProject {
  string id = 1;
  string name = 2;          // é¡¹ç›®å
  string difficulty = 3;    // éš¾åº¦
  repeated string tech_tags = 4;  // æŠ€æœ¯æ ‡ç­¾
  string description = 5;   // é¡¹ç›®ç®€è¿°
  string requirements = 6;  // äº§å‡ºè¦æ±‚
  optional string pdf_url = 7;    // ç”³è¯·ä¹¦PDFé“¾æ¥
}

message OsppProjectResponse {
  repeated OsppProject projects = 1;
  int32 total = 2;
  string message = 3;
}

message StartOsppCrawlerRequest {
  bool download_pdf = 1;    // æ˜¯å¦ä¸‹è½½PDF
}

// ==================== å¯¼å‡ºæ¥å£ ====================

message ExportToCsvRequest {
  string crawler_type = 1;  // "fzu" | "zhihu" | "ospp"
  optional string output_path = 2;
}

message ExportToCsvResponse {
  bool success = 1;
  string file_path = 2;
  string message = 3;
}

// ==================== æœåŠ¡å®šä¹‰ ====================

service CrawlerService {
  // ç¦å¤§æ•™åŠ¡é€šçŸ¥
  rpc StartFzuCrawler(StartFzuCrawlerRequest) returns (CrawlerResult);
  rpc GetFzuNotices(FzuNoticeRequest) returns (FzuNoticeResponse);
  
  // çŸ¥ä¹è¯é¢˜
  rpc StartZhihuCrawler(StartZhihuCrawlerRequest) returns (CrawlerResult);
  rpc GetZhihuTopics(ZhihuTopicRequest) returns (ZhihuTopicResponse);
  
  // å¼€æºä¹‹å¤
  rpc StartOsppCrawler(StartOsppCrawlerRequest) returns (CrawlerResult);
  rpc GetOsppProjects(OsppProjectRequest) returns (OsppProjectResponse);
  
  // é€šç”¨å¯¼å‡º
  rpc ExportToCsv(ExportToCsvRequest) returns (ExportToCsvResponse);
}
```

## æ•°æ®åº“æ¨¡å‹è®¾è®¡

### åŸºç¡€æ¨¡å‹ (models/base.py)

```python
from sqlalchemy import Column, Integer, DateTime
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func

Base = declarative_base()

class BaseModel(Base):
    """åŸºç¡€æ¨¡å‹ç±»"""
    __abstract__ = True
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
```

### ç¦å¤§é€šçŸ¥æ¨¡å‹ (models/fzu_notice.py)

```python
from sqlalchemy import Column, String, Text, Integer, ForeignKey
from sqlalchemy.orm import relationship
from models.base import BaseModel

class FzuNotice(BaseModel):
    __tablename__ = "fzu_notices"
    
    publisher = Column(String(100), nullable=False, comment="é€šçŸ¥äºº")
    title = Column(String(500), nullable=False, comment="æ ‡é¢˜")
    publish_date = Column(String(20), nullable=False, comment="å‘å¸ƒæ—¥æœŸ")
    detail_url = Column(String(500), nullable=False, unique=True, comment="è¯¦æƒ…é“¾æ¥")
    html_content = Column(Text, comment="è¯¦æƒ…HTMLå†…å®¹")
    
    # å…³è”é™„ä»¶
    attachments = relationship("FzuNoticeAttachment", back_populates="notice", 
                              cascade="all, delete-orphan")

class FzuNoticeAttachment(BaseModel):
    __tablename__ = "fzu_notice_attachments"
    
    notice_id = Column(Integer, ForeignKey("fzu_notices.id"), nullable=False)
    name = Column(String(500), nullable=False, comment="é™„ä»¶å")
    download_count = Column(Integer, default=0, comment="ä¸‹è½½æ¬¡æ•°")
    url = Column(String(500), nullable=False, comment="é™„ä»¶é“¾æ¥ç ")
    local_path = Column(String(1000), comment="æœ¬åœ°å­˜å‚¨è·¯å¾„")
    
    # å…³è”é€šçŸ¥
    notice = relationship("FzuNotice", back_populates="attachments")
```

### çŸ¥ä¹è¯é¢˜æ¨¡å‹ (models/zhihu_topic.py)

```python
from sqlalchemy import Column, String, Text, Integer, ForeignKey
from sqlalchemy.orm import relationship
from models.base import BaseModel

class ZhihuQuestion(BaseModel):
    __tablename__ = "zhihu_questions"
    
    question_id = Column(String(50), unique=True, nullable=False, comment="é—®é¢˜ID")
    topic_id = Column(String(50), nullable=False, comment="è¯é¢˜ID")
    title = Column(String(500), nullable=False, comment="é—®é¢˜æ ‡é¢˜")
    content = Column(Text, comment="é—®é¢˜è¯¦ç»†å†…å®¹")
    
    # å…³è”å›ç­”
    answers = relationship("ZhihuAnswer", back_populates="question",
                          cascade="all, delete-orphan")

class ZhihuAnswer(BaseModel):
    __tablename__ = "zhihu_answers"
    
    question_id = Column(Integer, ForeignKey("zhihu_questions.id"), nullable=False)
    answer_id = Column(String(50), unique=True, nullable=False, comment="å›ç­”ID")
    author = Column(String(200), comment="ä½œè€…")
    content = Column(Text, nullable=False, comment="å›ç­”å†…å®¹(çº¯æ–‡æœ¬)")
    vote_count = Column(Integer, default=0, comment="èµåŒæ•°")
    
    # å…³è”é—®é¢˜
    question = relationship("ZhihuQuestion", back_populates="answers")
```

### å¼€æºä¹‹å¤æ¨¡å‹ (models/ospp_project.py)

```python
from sqlalchemy import Column, String, Text, ARRAY
from models.base import BaseModel

class OsppProject(BaseModel):
    __tablename__ = "ospp_projects"
    
    project_id = Column(String(50), unique=True, nullable=False, comment="é¡¹ç›®ID")
    name = Column(String(500), nullable=False, comment="é¡¹ç›®å")
    difficulty = Column(String(50), comment="éš¾åº¦")
    tech_tags = Column(ARRAY(String), comment="æŠ€æœ¯æ ‡ç­¾åˆ—è¡¨")
    description = Column(Text, comment="é¡¹ç›®ç®€è¿°")
    requirements = Column(Text, comment="äº§å‡ºè¦æ±‚")
    pdf_url = Column(String(500), comment="ç”³è¯·ä¹¦PDFé“¾æ¥")
    pdf_local_path = Column(String(1000), comment="PDFæœ¬åœ°è·¯å¾„")
```

## API æ¥å£å®ç°ç¤ºä¾‹

### FastAPI è·¯ç”± (api/v1/fzu_notice.py)

```python
from fastapi import APIRouter, Depends, BackgroundTasks
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Optional
from api.deps import get_db
from schemas.fzu_notice import FzuNoticeRequest, FzuNoticeResponse, StartFzuCrawlerRequest
from crawlers.fzu_notice import FzuNoticeCrawler
from tasks.crawler_tasks import start_fzu_crawler_task

router = APIRouter(prefix="/fzu", tags=["ç¦å¤§æ•™åŠ¡é€šçŸ¥"])

@router.post("/start-crawler")
async def start_crawler(
    request: StartFzuCrawlerRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db)
):
    """å¯åŠ¨ç¦å¤§é€šçŸ¥çˆ¬è™« (å¼‚æ­¥ä»»åŠ¡)"""
    # ä½¿ç”¨ Celery å¼‚æ­¥ä»»åŠ¡
    task = start_fzu_crawler_task.delay(request.target_count)
    return {
        "task_id": task.id,
        "message": "Crawler task started"
    }

@router.get("/notices", response_model=FzuNoticeResponse)
async def get_notices(
    page: int = 1,
    page_size: int = 20,
    keyword: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
):
    """æŸ¥è¯¢ç¦å¤§é€šçŸ¥åˆ—è¡¨"""
    # å®ç°åˆ†é¡µæŸ¥è¯¢é€»è¾‘
    pass

@router.get("/notices/{notice_id}")
async def get_notice_detail(
    notice_id: int,
    db: AsyncSession = Depends(get_db)
):
    """è·å–é€šçŸ¥è¯¦æƒ…"""
    pass

@router.post("/export-csv")
async def export_to_csv(
    output_path: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
):
    """å¯¼å‡ºä¸º CSV"""
    pass
```

## å…·ä½“çˆ¬è™«å®ç°ç¤ºä¾‹

### ç¦å¤§æ•™åŠ¡é€šçŸ¥çˆ¬è™« (crawlers/fzu_notice.py)

```python
from typing import List, Dict, Any
from lxml import etree
from core.http_crawler import HttpCrawler
from models.fzu_notice import FzuNotice, FzuNoticeAttachment
from core.parser import Parser

class FzuNoticeCrawler(HttpCrawler):
    """ç¦å¤§æ•™åŠ¡é€šçŸ¥çˆ¬è™«"""
    
    BASE_URL = "https://jwch.fzu.edu.cn"
    LIST_URL = "https://jwch.fzu.edu.cn/jxtz.htm"
    
    async def fetch(self, target_count: int = 500, **kwargs) -> List[Dict[str, Any]]:
        """
        è·å–é€šçŸ¥åˆ—è¡¨
        
        å®ç°è¦ç‚¹:
        1. åˆ†æç½‘é¡µåˆ†é¡µè§„å¾‹ (jxtz.htm, jxtz/list2.htm, ...)
        2. å¾ªç¯è¯·æ±‚æ¯ä¸€é¡µ
        3. ä½¿ç”¨ XPath æå–é€šçŸ¥åˆ—è¡¨
        """
        notices = []
        page = 0
        
        while len(notices) < target_count:
            if page == 0:
                url = self.LIST_URL
            else:
                url = f"{self.BASE_URL}/jxtz/list{page + 1}.htm"
            
            response = await self.get(url)
            html = response.text
            
            # XPath æå–åˆ—è¡¨é¡¹
            tree = etree.HTML(html)
            items = tree.xpath('//ul[@class="news_list list-left"]/li')
            
            if not items:
                break
            
            for item in items:
                notice_data = {
                    'title': item.xpath('.//a/@title')[0] if item.xpath('.//a/@title') else '',
                    'detail_url': self.BASE_URL + item.xpath('.//a/@href')[0],
                    'date': item.xpath('.//span[@class="news_meta"]/text()')[0].strip(),
                    'publisher': self._extract_publisher(item.xpath('.//a/@title')[0])
                }
                notices.append(notice_data)
            
            page += 1
        
        return notices[:target_count]
    
    def _extract_publisher(self, title: str) -> str:
        """ä»æ ‡é¢˜ä¸­æå–å‘å¸ƒè€… (è´¨é‡åŠã€è®¡åˆ’ç§‘ç­‰)"""
        # ä½¿ç”¨æ­£åˆ™æˆ–å­—ç¬¦ä¸²å¤„ç†æå–
        import re
        match = re.search(r'ã€(.+?)ã€‘', title)
        return match.group(1) if match else "æœªçŸ¥"
    
    async def parse(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æå•æ¡é€šçŸ¥è¯¦æƒ…
        
        å®ç°è¦ç‚¹:
        1. è¯·æ±‚è¯¦æƒ…é¡µ
        2. æå–æ­£æ–‡HTML
        3. æå–é™„ä»¶ä¿¡æ¯ (åç§°ã€ä¸‹è½½æ¬¡æ•°ã€é“¾æ¥)
        4. æ¸…ç†æ–‡æœ¬
        """
        detail_url = raw_data['detail_url']
        response = await self.get(detail_url)
        html = response.text
        tree = etree.HTML(html)
        
        # æå–æ­£æ–‡
        content = tree.xpath('//div[@class="wp_articlecontent"]')[0]
        html_content = etree.tostring(content, encoding='unicode')
        
        # æå–é™„ä»¶
        attachments = []
        attachment_items = tree.xpath('//div[@class="wp_article_attach_list"]//li')
        for item in attachment_items:
            att = {
                'name': Parser.clean_text(item.xpath('.//a/text()')[0]),
                'url': item.xpath('.//a/@href')[0],
                'download_count': self._extract_download_count(item)
            }
            attachments.append(att)
        
        return {
            **raw_data,
            'html_content': html_content,
            'attachments': attachments
        }
    
    def _extract_download_count(self, item) -> int:
        """æå–ä¸‹è½½æ¬¡æ•°"""
        # ä»é¡µé¢ä¸­æå–ä¸‹è½½æ¬¡æ•°é€»è¾‘
        pass
    
    async def save(self, data: Dict[str, Any]) -> None:
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        notice = FzuNotice(
            publisher=data['publisher'],
            title=Parser.clean_text(data['title']),
            publish_date=data['date'],
            detail_url=data['detail_url'],
            html_content=data['html_content']
        )
        
        self.db_session.add(notice)
        await self.db_session.flush()  # è·å– notice.id
        
        # ä¿å­˜é™„ä»¶
        for att_data in data['attachments']:
            attachment = FzuNoticeAttachment(
                notice_id=notice.id,
                name=att_data['name'],
                download_count=att_data['download_count'],
                url=att_data['url']
            )
            self.db_session.add(attachment)
```

### çŸ¥ä¹è¯é¢˜çˆ¬è™« (crawlers/zhihu_topic.py)

```python
from typing import List, Dict, Any
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from core.selenium_crawler import SeleniumCrawler
from models.zhihu_topic import ZhihuQuestion, ZhihuAnswer
import time

class ZhihuTopicCrawler(SeleniumCrawler):
    """çŸ¥ä¹è¯é¢˜çˆ¬è™«"""
    
    async def fetch(self, topic_url: str, question_count: int = 20, **kwargs) -> List[Dict[str, Any]]:
        """
        è·å–è¯é¢˜ä¸‹çš„é—®é¢˜åˆ—è¡¨
        
        å®ç°è¦ç‚¹:
        1. ä½¿ç”¨ Selenium æ¨¡æ‹Ÿæµè§ˆå™¨
        2. å¤„ç†ç™»å½• (Cookie æˆ–æ‰«ç )
        3. æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
        4. æå–é—®é¢˜é“¾æ¥
        """
        self.driver.get(topic_url)
        time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
        
        # TODO: å¤„ç†ç™»å½• (å¯ä»¥æå‰ä¿å­˜ Cookie)
        
        questions = []
        scroll_count = 0
        max_scrolls = 10
        
        while len(questions) < question_count and scroll_count < max_scrolls:
            # æå–å½“å‰é¡µé¢çš„é—®é¢˜
            question_elements = self.driver.find_elements(By.CSS_SELECTOR, '.List-item')
            
            for elem in question_elements:
                if len(questions) >= question_count:
                    break
                
                try:
                    question_data = {
                        'title': elem.find_element(By.CSS_SELECTOR, 'h2').text,
                        'url': elem.find_element(By.CSS_SELECTOR, 'h2 a').get_attribute('href')
                    }
                    if question_data not in questions:
                        questions.append(question_data)
                except:
                    continue
            
            # æ»šåŠ¨åŠ è½½æ›´å¤š
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            scroll_count += 1
        
        return questions[:question_count]
    
    async def parse(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æé—®é¢˜è¯¦æƒ…å’Œå›ç­”
        
        å®ç°è¦ç‚¹:
        1. è®¿é—®é—®é¢˜é¡µé¢
        2. æå–é—®é¢˜è¯¦ç»†å†…å®¹
        3. æå–å›ç­”åˆ—è¡¨ (é™åˆ¶æ•°é‡)
        4. æå–å›ç­”çš„çº¯æ–‡æœ¬å†…å®¹
        """
        question_url = raw_data['url']
        self.driver.get(question_url)
        time.sleep(2)
        
        # æå–é—®é¢˜å†…å®¹
        try:
            content_elem = self.driver.find_element(By.CSS_SELECTOR, '.QuestionRichText')
            content = content_elem.text
        except:
            content = ""
        
        # æå–å›ç­”
        answers = []
        answer_elements = self.driver.find_elements(By.CSS_SELECTOR, '.List-item')[:10]
        
        for elem in answer_elements:
            try:
                answer_data = {
                    'author': elem.find_element(By.CSS_SELECTOR, '.AuthorInfo-name').text,
                    'content': elem.find_element(By.CSS_SELECTOR, '.RichContent-inner').text,
                    'vote_count': self._extract_vote_count(elem)
                }
                answers.append(answer_data)
            except:
                continue
        
        return {
            'title': raw_data['title'],
            'content': content,
            'answers': answers,
            'question_id': self._extract_question_id(question_url)
        }
    
    def _extract_vote_count(self, element) -> int:
        """æå–èµåŒæ•°"""
        try:
            vote_text = element.find_element(By.CSS_SELECTOR, '.VoteButton--up').text
            return int(vote_text) if vote_text.isdigit() else 0
        except:
            return 0
    
    def _extract_question_id(self, url: str) -> str:
        """ä»URLæå–é—®é¢˜ID"""
        import re
        match = re.search(r'/question/(\d+)', url)
        return match.group(1) if match else ""
    
    async def save(self, data: Dict[str, Any]) -> None:
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        question = ZhihuQuestion(
            question_id=data['question_id'],
            topic_id=data.get('topic_id', ''),
            title=data['title'],
            content=data['content']
        )
        
        self.db_session.add(question)
        await self.db_session.flush()
        
        # ä¿å­˜å›ç­”
        for ans_data in data['answers']:
            answer = ZhihuAnswer(
                question_id=question.id,
                answer_id=ans_data.get('answer_id', ''),
                author=ans_data['author'],
                content=ans_data['content'],
                vote_count=ans_data['vote_count']
            )
            self.db_session.add(answer)
```

### å¼€æºä¹‹å¤çˆ¬è™« (crawlers/ospp_project.py)

```python
from typing import List, Dict, Any
import json
from core.http_crawler import HttpCrawler
from models.ospp_project import OsppProject

class OsppProjectCrawler(HttpCrawler):
    """å¼€æºä¹‹å¤é¡¹ç›®çˆ¬è™«"""
    
    API_URL = "https://summer-ospp.ac.cn/api/projects"  # å‡è®¾çš„APIåœ°å€
    
    async def fetch(self, **kwargs) -> List[Dict[str, Any]]:
        """
        é€šè¿‡æŠ“åŒ…æ‰¾åˆ°çš„APIè·å–é¡¹ç›®åˆ—è¡¨
        
        å®ç°è¦ç‚¹:
        1. ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·æ‰¾åˆ°APIæ¥å£
        2. åˆ†æè¯·æ±‚å‚æ•° (åˆ†é¡µã€ç­›é€‰ç­‰)
        3. æ¨¡æ‹Ÿè¯·æ±‚è·å–JSONæ•°æ®
        """
        all_projects = []
        page = 1
        page_size = 50
        
        while True:
            params = {
                'page': page,
                'pageSize': page_size,
                'year': 2025
            }
            
            response = await self.get(self.API_URL, params=params)
            data = response.json()
            
            projects = data.get('data', {}).get('list', [])
            if not projects:
                break
            
            all_projects.extend(projects)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
            if len(projects) < page_size:
                break
            
            page += 1
        
        return all_projects
    
    async def parse(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æé¡¹ç›®è¯¦æƒ…
        
        å®ç°è¦ç‚¹:
        1. æå–é¡¹ç›®åŸºæœ¬ä¿¡æ¯
        2. å¦‚æœéœ€è¦æ›´è¯¦ç»†ä¿¡æ¯,è¯·æ±‚é¡¹ç›®è¯¦æƒ…API
        3. æå–PDFé“¾æ¥ (è¿›é˜¶)
        """
        project_id = raw_data.get('id')
        
        # è¯·æ±‚è¯¦æƒ…API (å¦‚æœæœ‰)
        detail_url = f"{self.API_URL}/{project_id}"
        response = await self.get(detail_url)
        detail_data = response.json().get('data', {})
        
        return {
            'project_id': str(project_id),
            'name': detail_data.get('name', ''),
            'difficulty': detail_data.get('difficulty', ''),
            'tech_tags': detail_data.get('techTags', []),
            'description': detail_data.get('description', ''),
            'requirements': detail_data.get('requirements', ''),
            'pdf_url': detail_data.get('pdfUrl', '')
        }
    
    async def save(self, data: Dict[str, Any]) -> None:
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        project = OsppProject(
            project_id=data['project_id'],
            name=data['name'],
            difficulty=data['difficulty'],
            tech_tags=data['tech_tags'],
            description=data['description'],
            requirements=data['requirements'],
            pdf_url=data['pdf_url']
        )
        
        self.db_session.add(project)
```

## é…ç½®æ–‡ä»¶

### åº”ç”¨é…ç½® (config/settings.py)

```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # åº”ç”¨é…ç½®
    APP_NAME: str = "Crawler Platform"
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = True
    
    # æ•°æ®åº“é…ç½®
    DATABASE_URL: str = "postgresql+asyncpg://user:pass@localhost:5432/crawler"
    
    # Redis é…ç½®
    REDIS_URL: str = "redis://localhost:6379/0"
    
    # Celery é…ç½®
    CELERY_BROKER_URL: str = "redis://localhost:6379/1"
    CELERY_RESULT_BACKEND: str = "redis://localhost:6379/2"
    
    # çˆ¬è™«é…ç½®
    REQUEST_TIMEOUT: int = 30
    MAX_RETRIES: int = 3
    DOWNLOAD_DIR: str = "./downloads"
    
    # Selenium é…ç½®
    SELENIUM_HEADLESS: bool = True
    CHROME_DRIVER_PATH: str = "/usr/local/bin/chromedriver"
    
    class Config:
        env_file = ".env"

@lru_cache()
def get_settings() -> Settings:
    return Settings()
```

### æ•°æ®åº“é…ç½® (config/database.py)

```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from config.settings import get_settings

settings = get_settings()

# åˆ›å»ºå¼‚æ­¥å¼•æ“
engine = create_async_engine(
    settings.DATABASE_URL,
    echo=settings.DEBUG,
    future=True,
    pool_pre_ping=True,
    pool_size=10,
    max_overflow=20
)

# åˆ›å»ºå¼‚æ­¥ä¼šè¯å·¥å‚
async_session_factory = sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autocommit=False,
    autoflush=False
)

async def get_db() -> AsyncSession:
    """ä¾èµ–æ³¨å…¥: è·å–æ•°æ®åº“ä¼šè¯"""
    async with async_session_factory() as session:
        try:
            yield session
        finally:
            await session.close()
```

## Celery ä»»åŠ¡é…ç½® (tasks/celery_app.py)

```python
from celery import Celery
from config.settings import get_settings

settings = get_settings()

celery_app = Celery(
    "crawler_tasks",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Asia/Shanghai',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=3600,  # 1å°æ—¶è¶…æ—¶
    task_soft_time_limit=3300,  # 55åˆ†é’Ÿè½¯è¶…æ—¶
)
```

## å¼€å‘æµç¨‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # macOS/Linux

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£… PostgreSQL (ä½¿ç”¨ Homebrew)
brew install postgresql@14
brew services start postgresql@14

# åˆ›å»ºæ•°æ®åº“
createdb crawler

# å®‰è£… Redis
brew install redis
brew services start redis
```

### 2. åˆå§‹åŒ–æ•°æ®åº“

```bash
# ç”Ÿæˆè¿ç§»æ–‡ä»¶
alembic revision --autogenerate -m "Initial migration"

# æ‰§è¡Œè¿ç§»
alembic upgrade head
```

### 3. ç”Ÿæˆ Protobuf ä»£ç 

```bash
# å®‰è£…å·¥å…·
pip install grpcio-tools protobuf-to-pydantic

# ç”Ÿæˆ Python ä»£ç 
python -m grpc_tools.protoc \
    -I./proto \
    --python_out=./proto/generated \
    --grpc_python_out=./proto/generated \
    ./proto/crawler.proto

# è½¬æ¢ä¸º Pydantic æ¨¡å‹ (ç”¨äº FastAPI)
# æ‰‹åŠ¨æˆ–ä½¿ç”¨å·¥å…·è½¬æ¢
```

### 4. å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨ FastAPI åº”ç”¨
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# å¯åŠ¨ Celery Worker
celery -A tasks.celery_app worker --loglevel=info

# å¯åŠ¨ Celery Beat (å®šæ—¶ä»»åŠ¡)
celery -A tasks.celery_app beat --loglevel=info
```

### 5. æµ‹è¯•çˆ¬è™«

```bash
# ä½¿ç”¨ curl æˆ– httpie æµ‹è¯•
curl -X POST http://localhost:8000/api/v1/fzu/start-crawler \
  -H "Content-Type: application/json" \
  -d '{"target_count": 500}'

# æŸ¥è¯¢ç»“æœ
curl http://localhost:8000/api/v1/fzu/notices?page=1&page_size=20
```

## ä¾èµ–æ¸…å• (requirements.txt)

```txt
# Web æ¡†æ¶
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0

# æ•°æ®åº“
sqlalchemy[asyncio]==2.0.23
asyncpg==0.29.0
alembic==1.13.0
psycopg2-binary==2.9.9

# ORM å·¥å…·
sqlacodegen==3.0.0rc5

# çˆ¬è™«ç›¸å…³
requests==2.31.0
httpx==0.25.2
lxml==4.9.3
beautifulsoup4==4.12.2
selenium==4.15.2
webdriver-manager==4.0.1

# ä»»åŠ¡é˜Ÿåˆ—
celery==5.3.4
redis==5.0.1

# Protobuf
protobuf==4.25.1
grpcio==1.60.0
grpcio-tools==1.60.0

# å·¥å…·åº“
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-dotenv==1.0.0

# æµ‹è¯•
pytest==7.4.3
pytest-asyncio==0.21.1
httpx==0.25.2

# æ—¥å¿—
loguru==0.7.2
```

## å¼€å‘å»ºè®®

### 1. ä»£ç ç»„ç»‡
- ä¿æŒçˆ¬è™«é€»è¾‘ç‹¬ç«‹,ä¾¿äºå¤ç”¨
- ä½¿ç”¨ä¾èµ–æ³¨å…¥ç®¡ç†æ•°æ®åº“è¿æ¥
- éµå¾ªå•ä¸€èŒè´£åŸåˆ™

### 2. é”™è¯¯å¤„ç†
- ç½‘ç»œè¯·æ±‚æ·»åŠ é‡è¯•æœºåˆ¶
- è§£æå¤±è´¥æ—¶è®°å½•æ—¥å¿—ä½†ä¸ä¸­æ–­æµç¨‹
- ä½¿ç”¨ try-except åŒ…è£¹å…³é”®ä»£ç 

### 3. æ€§èƒ½ä¼˜åŒ–
- ä½¿ç”¨å¼‚æ­¥ I/O (async/await)
- åˆç†ä½¿ç”¨è¿æ¥æ± 
- é¿å…åœ¨å¾ªç¯ä¸­è¿›è¡Œæ•°æ®åº“æ“ä½œ

### 4. æ•°æ®æ¸…æ´—
- ç»Ÿä¸€çš„æ–‡æœ¬æ¸…æ´—å‡½æ•°
- å»é™¤ HTML æ ‡ç­¾
- å¤„ç†ç‰¹æ®Šå­—ç¬¦

### 5. åçˆ¬åº”å¯¹
- è®¾ç½®åˆç†çš„è¯·æ±‚é—´éš”
- ä½¿ç”¨ä»£ç†æ±  (å¦‚éœ€è¦)
- è½®æ¢ User-Agent
- å¤„ç† Cookie å’Œ Session

### 6. æ—¥å¿—è®°å½•
- ä½¿ç”¨ loguru è®°å½•è¯¦ç»†æ—¥å¿—
- åŒºåˆ†ä¸åŒçº§åˆ« (DEBUG, INFO, ERROR)
- è®°å½•å…³é”®æ“ä½œå’Œå¼‚å¸¸

## CSV å¯¼å‡ºç¤ºä¾‹ (utils/csv_writer.py)

```python
import csv
from typing import List, Dict, Any
from pathlib import Path

class CsvWriter:
    """CSV å¯¼å‡ºå·¥å…·"""
    
    @staticmethod
    async def write_fzu_notices(notices: List[Dict[str, Any]], output_path: str):
        """å¯¼å‡ºç¦å¤§é€šçŸ¥åˆ° CSV"""
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['ID', 'é€šçŸ¥äºº', 'æ ‡é¢˜', 'æ—¥æœŸ', 'è¯¦æƒ…é“¾æ¥', 'é™„ä»¶æ•°é‡']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            
            writer.writeheader()
            for notice in notices:
                writer.writerow({
                    'ID': notice['id'],
                    'é€šçŸ¥äºº': notice['publisher'],
                    'æ ‡é¢˜': notice['title'],
                    'æ—¥æœŸ': notice['publish_date'],
                    'è¯¦æƒ…é“¾æ¥': notice['detail_url'],
                    'é™„ä»¶æ•°é‡': len(notice.get('attachments', []))
                })
    
    @staticmethod
    async def write_zhihu_topics(questions: List[Dict[str, Any]], output_path: str):
        """å¯¼å‡ºçŸ¥ä¹è¯é¢˜åˆ° CSV"""
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['é—®é¢˜ID', 'é—®é¢˜æ ‡é¢˜', 'é—®é¢˜å†…å®¹', 'å›ç­”æ•°', 'å›ç­”å†…å®¹']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            
            writer.writeheader()
            for question in questions:
                for answer in question.get('answers', []):
                    writer.writerow({
                        'é—®é¢˜ID': question['question_id'],
                        'é—®é¢˜æ ‡é¢˜': question['title'],
                        'é—®é¢˜å†…å®¹': question['content'],
                        'å›ç­”æ•°': len(question.get('answers', [])),
                        'å›ç­”å†…å®¹': answer['content']
                    })
```

## æ€»ç»“

è¿™ä¸ªé¡¹ç›®æ¶æ„å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹:

1. **é«˜åº¦æ¨¡å—åŒ–**: æ ¸å¿ƒçˆ¬è™«å¼•æ“ä¸å…·ä½“å®ç°åˆ†ç¦»
2. **å¯æ‰©å±•æ€§å¼º**: æ–°å¢çˆ¬è™«åªéœ€ç»§æ‰¿åŸºç±»å¹¶å®ç°ä¸‰ä¸ªæ–¹æ³•
3. **æŠ€æœ¯æ ˆç°ä»£**: ä½¿ç”¨ FastAPI + SQLAlchemy 2.0 + async/await
4. **æ¥å£è§„èŒƒ**: é€šè¿‡ Protobuf å®šä¹‰æ¸…æ™°çš„æ¥å£å¥‘çº¦
5. **ç”Ÿäº§å°±ç»ª**: åŒ…å«å¼‚æ­¥ä»»åŠ¡ã€æ•°æ®åº“è¿ç§»ã€é”™è¯¯å¤„ç†ç­‰å®Œæ•´åŠŸèƒ½

## ä¸‹ä¸€æ­¥

1. å®Œå–„é…ç½®æ–‡ä»¶ (.env)
2. å®ç°å…·ä½“çš„çˆ¬è™«é€»è¾‘
3. ç¼–å†™å•å…ƒæµ‹è¯•
4. æ·»åŠ  API æ–‡æ¡£ (FastAPI è‡ªåŠ¨ç”Ÿæˆ)
5. éƒ¨ç½²ä¸Šçº¿ (Docker + Nginx)

æœ‰é—®é¢˜éšæ—¶æé—®! ğŸš€
